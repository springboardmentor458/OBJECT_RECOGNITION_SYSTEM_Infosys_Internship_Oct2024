{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5qx8OsPcEQhC",
    "outputId": "0ce36026-3559-4c2b-9ab6-bc2a9dc6b975"
   },
   "outputs": [],
   "source": [
    "!pip install -U albumentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "f1tcm619Ntxx"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "SE_nySCqNnLE"
   },
   "outputs": [],
   "source": [
    "json_path =  \"C:/Users/megha/Downloads/instances_val2017.json-20241115T150718Z-001/instances_val2017.json/instances_val2017.json\"\n",
    "images_dir = \"C:/Users/megha/Downloads/val2017-20241115T150951Z-001/val2017\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    " class CocoDataset(torch.utils.data.Dataset):  # Inheriting from Dataset\n",
    "    def __init__(self, image_metadata, annotation_data, category_mapping, image_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Initializes the dataset with images, annotations, and other required data.\n",
    "        \n",
    "        Parameters:\n",
    "            image_metadata (list): A list of image metadata (e.g., file names and image IDs).\n",
    "            annotation_data (list): A list of annotations (bounding boxes and category IDs).\n",
    "            category_mapping (dict): A mapping of category IDs to class names.\n",
    "            image_dir (str): Directory where images are stored.\n",
    "            transform (callable, optional): Transformation to apply to images and annotations.\n",
    "        \"\"\"\n",
    "        self.image_metadata = image_metadata\n",
    "        self.annotation_data = annotation_data\n",
    "        self.category_mapping = category_mapping\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.image_id_to_annotations = self._group_annotations_by_image()\n",
    "\n",
    "    def _group_annotations_by_image(self):\n",
    "        \"\"\"\n",
    "        Groups annotations by image ID for efficient retrieval during data loading.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary mapping image IDs to their annotations.\n",
    "        \"\"\"\n",
    "        image_id_to_annotations = {}\n",
    "        for annotation in self.annotation_data:\n",
    "            image_id = annotation['image_id']\n",
    "            if image_id not in image_id_to_annotations:\n",
    "                image_id_to_annotations[image_id] = []\n",
    "            image_id_to_annotations[image_id].append(annotation)\n",
    "        return image_id_to_annotations\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the number of images in the dataset.\n",
    "\n",
    "        Returns:\n",
    "            int: Total number of images.\n",
    "        \"\"\"\n",
    "        return len(self.image_metadata)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retrieves an image and its corresponding annotations.\n",
    "\n",
    "        Parameters:\n",
    "            idx (int): Index of the image and its annotations.\n",
    "\n",
    "        Returns:\n",
    "            tuple: A tuple containing the image and its target annotations (bounding boxes and labels).\n",
    "        \"\"\"\n",
    "        # Get image information\n",
    "        image_info = self.image_metadata[idx]\n",
    "        img_path = os.path.join(self.image_dir, image_info['file_name'])\n",
    "        \n",
    "        # Print the image path to check if it is correct\n",
    "        print(f\"Attempting to load image from: {img_path}\")\n",
    "        \n",
    "        # Read the image\n",
    "        image = cv2.imread(img_path)\n",
    "        \n",
    "        # Check if the image is loaded correctly\n",
    "        if image is None:\n",
    "            raise FileNotFoundError(f\"Image not found or failed to load at path: {img_path}\")\n",
    "        \n",
    "        # Convert the image to RGB format\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Retrieve annotations for the image\n",
    "        image_id = image_info['id']\n",
    "        annotations = self.image_id_to_annotations.get(image_id, [])\n",
    "        \n",
    "        # Initialize lists for bounding boxes and labels\n",
    "        bounding_boxes = []\n",
    "        category_labels = []\n",
    "        for annotation in annotations:\n",
    "            x, y, width, height = annotation['bbox']\n",
    "            bounding_boxes.append([x, y, x + width, y + height])  # Convert to (x1, y1, x2, y2)\n",
    "            category_labels.append(annotation['category_id'])\n",
    "        \n",
    "        # Convert bounding boxes and labels to numpy arrays for transformation\n",
    "        bounding_boxes = np.array(bounding_boxes)\n",
    "        category_labels = np.array(category_labels)\n",
    "        \n",
    "        # Apply transformations if provided\n",
    "        if self.transform:\n",
    "            transformed = self.transform(image=image, bboxes=bounding_boxes, labels=category_labels)\n",
    "            image = transformed['image']\n",
    "            bounding_boxes = transformed['bboxes']\n",
    "            category_labels = transformed['labels']\n",
    "        \n",
    "        # Convert to PyTorch tensors\n",
    "        bounding_boxes = torch.tensor(bounding_boxes, dtype=torch.float32)\n",
    "        category_labels = torch.tensor(category_labels, dtype=torch.int64)\n",
    "        target = {\"boxes\": bounding_boxes, \"labels\": category_labels}\n",
    "        \n",
    "        return image, target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "rhQgpoVoQ6Yw"
   },
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mGXrtxPaIJlP",
    "outputId": "bd2519f8-6f5e-47b3-eb93-fa6abfdcbbe3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to load image from: C:/Users/megha/Downloads/val2017-20241115T150951Z-001/val2017\\000000380706.jpg\n",
      "Attempting to load image from: C:/Users/megha/Downloads/val2017-20241115T150951Z-001/val2017\\000000215072.jpg\n",
      "Attempting to load image from: C:/Users/megha/Downloads/val2017-20241115T150951Z-001/val2017\\000000570736.jpg\n",
      "Attempting to load image from: C:/Users/megha/Downloads/val2017-20241115T150951Z-001/val2017\\000000375493.jpg\n",
      "Attempting to load image from: C:/Users/megha/Downloads/val2017-20241115T150951Z-001/val2017\\000000373382.jpg\n",
      "Attempting to load image from: C:/Users/megha/Downloads/val2017-20241115T150951Z-001/val2017\\000000479099.jpg\n",
      "Attempting to load image from: C:/Users/megha/Downloads/val2017-20241115T150951Z-001/val2017\\000000430048.jpg\n",
      "Attempting to load image from: C:/Users/megha/Downloads/val2017-20241115T150951Z-001/val2017\\000000100723.jpg\n",
      "Batch 0 - Image batch size: torch.Size([8, 3, 416, 416])\n",
      "Batch 0 - Number of targets: 8\n",
      "Sample target: {'boxes': tensor([[203.2693, 271.1041, 263.4796, 388.2067],\n",
      "        [131.1933, 101.1069, 219.6444, 192.8449],\n",
      "        [174.6999, 161.9413, 267.5446, 373.2140],\n",
      "        [  0.0000, 167.4838,  21.1635, 291.4786],\n",
      "        [  8.7668, 151.7324,  50.1073, 266.8020],\n",
      "        [ 92.6662, 155.7462, 111.5140, 175.1370],\n",
      "        [ 52.7393, 133.5296,  96.2633, 180.1360],\n",
      "        [  8.5432, 175.6780,  17.5376, 196.5468],\n",
      "        [ 40.9445, 142.3815,  44.5906, 148.9514]]), 'labels': tensor([ 4,  6,  1,  1,  1,  3,  6,  4, 10])}\n",
      "Attempting to load image from: C:/Users/megha/Downloads/val2017-20241115T150951Z-001/val2017\\000000572408.jpg\n",
      "Attempting to load image from: C:/Users/megha/Downloads/val2017-20241115T150951Z-001/val2017\\000000560011.jpg\n",
      "Attempting to load image from: C:/Users/megha/Downloads/val2017-20241115T150951Z-001/val2017\\000000217753.jpg\n",
      "Attempting to load image from: C:/Users/megha/Downloads/val2017-20241115T150951Z-001/val2017\\000000119641.jpg\n",
      "Attempting to load image from: C:/Users/megha/Downloads/val2017-20241115T150951Z-001/val2017\\000000088951.jpg\n",
      "Attempting to load image from: C:/Users/megha/Downloads/val2017-20241115T150951Z-001/val2017\\000000169356.jpg\n",
      "Attempting to load image from: C:/Users/megha/Downloads/val2017-20241115T150951Z-001/val2017\\000000151480.jpg\n",
      "Attempting to load image from: C:/Users/megha/Downloads/val2017-20241115T150951Z-001/val2017\\000000226984.jpg\n",
      "Batch 1 - Image batch size: torch.Size([8, 3, 416, 416])\n",
      "Batch 1 - Number of targets: 8\n",
      "Sample target: {'boxes': tensor([[118.9132, 156.5590, 321.4777, 304.1740],\n",
      "        [176.2604, 287.3650, 382.2098, 353.8405],\n",
      "        [105.4324, 145.0735, 133.8068, 168.7530],\n",
      "        [160.8566, 149.3570, 190.1826, 165.0220],\n",
      "        [136.5540, 161.5705, 313.3147, 301.0085]]), 'labels': tensor([21, 21, 21, 21, 20])}\n",
      "Attempting to load image from: C:/Users/megha/Downloads/val2017-20241115T150951Z-001/val2017\\000000173044.jpg\n",
      "Attempting to load image from: C:/Users/megha/Downloads/val2017-20241115T150951Z-001/val2017\\000000108244.jpg\n",
      "Attempting to load image from: C:/Users/megha/Downloads/val2017-20241115T150951Z-001/val2017\\000000390826.jpg\n",
      "Attempting to load image from: C:/Users/megha/Downloads/val2017-20241115T150951Z-001/val2017\\000000058111.jpg\n",
      "Attempting to load image from: C:/Users/megha/Downloads/val2017-20241115T150951Z-001/val2017\\000000455597.jpg\n",
      "Attempting to load image from: C:/Users/megha/Downloads/val2017-20241115T150951Z-001/val2017\\000000336209.jpg\n",
      "Attempting to load image from: C:/Users/megha/Downloads/val2017-20241115T150951Z-001/val2017\\000000081988.jpg\n",
      "Attempting to load image from: C:/Users/megha/Downloads/val2017-20241115T150951Z-001/val2017\\000000421455.jpg\n",
      "Batch 2 - Image batch size: torch.Size([8, 3, 416, 416])\n",
      "Batch 2 - Number of targets: 8\n",
      "Sample target: {'boxes': tensor([[ 70.5499, 160.4908, 136.6612, 208.2821]]), 'labels': tensor([85])}\n",
      "Attempting to load image from: C:/Users/megha/Downloads/val2017-20241115T150951Z-001/val2017\\000000577584.jpg\n",
      "Attempting to load image from: C:/Users/megha/Downloads/val2017-20241115T150951Z-001/val2017\\000000515350.jpg\n",
      "Attempting to load image from: C:/Users/megha/Downloads/val2017-20241115T150951Z-001/val2017\\000000479912.jpg\n",
      "Attempting to load image from: C:/Users/megha/Downloads/val2017-20241115T150951Z-001/val2017\\000000199236.jpg\n",
      "Attempting to load image from: C:/Users/megha/Downloads/val2017-20241115T150951Z-001/val2017\\000000450686.jpg\n",
      "Attempting to load image from: C:/Users/megha/Downloads/val2017-20241115T150951Z-001/val2017\\000000064718.jpg\n",
      "Attempting to load image from: C:/Users/megha/Downloads/val2017-20241115T150951Z-001/val2017\\000000482436.jpg\n",
      "Attempting to load image from: C:/Users/megha/Downloads/val2017-20241115T150951Z-001/val2017\\000000167353.jpg\n",
      "Batch 3 - Image batch size: torch.Size([8, 3, 416, 416])\n",
      "Batch 3 - Number of targets: 8\n",
      "Sample target: {'boxes': tensor([[ 69.0771,  63.8458, 306.2445, 242.9065],\n",
      "        [188.8414, 177.7403, 354.1201, 275.5150]]), 'labels': tensor([78, 51])}\n",
      "Attempting to load image from: C:/Users/megha/Downloads/val2017-20241115T150951Z-001/val2017\\000000106912.jpg\n",
      "Attempting to load image from: C:/Users/megha/Downloads/val2017-20241115T150951Z-001/val2017\\000000492077.jpg\n",
      "Attempting to load image from: C:/Users/megha/Downloads/val2017-20241115T150951Z-001/val2017\\000000399764.jpg\n",
      "Attempting to load image from: C:/Users/megha/Downloads/val2017-20241115T150951Z-001/val2017\\000000448076.jpg\n",
      "Attempting to load image from: C:/Users/megha/Downloads/val2017-20241115T150951Z-001/val2017\\000000530061.jpg\n",
      "Attempting to load image from: C:/Users/megha/Downloads/val2017-20241115T150951Z-001/val2017\\000000270402.jpg\n",
      "Attempting to load image from: C:/Users/megha/Downloads/val2017-20241115T150951Z-001/val2017\\000000046031.jpg\n",
      "Attempting to load image from: C:/Users/megha/Downloads/val2017-20241115T150951Z-001/val2017\\000000261061.jpg\n",
      "Batch 4 - Image batch size: torch.Size([8, 3, 416, 416])\n",
      "Batch 4 - Number of targets: 8\n",
      "Sample target: {'boxes': tensor([[149.5655, 160.8246, 207.5842, 279.1605],\n",
      "        [160.6215, 261.3577, 212.7105, 287.9497]]), 'labels': tensor([ 1, 41])}\n",
      "Attempting to load image from: C:/Users/megha/Downloads/val2017-20241115T150951Z-001/val2017\\000000512330.jpg\n",
      "Attempting to load image from: C:/Users/megha/Downloads/val2017-20241115T150951Z-001/val2017\\000000221502.jpg\n",
      "Attempting to load image from: C:/Users/megha/Downloads/val2017-20241115T150951Z-001/val2017\\000000458054.jpg\n",
      "Attempting to load image from: C:/Users/megha/Downloads/val2017-20241115T150951Z-001/val2017\\000000403385.jpg\n",
      "Attempting to load image from: C:/Users/megha/Downloads/val2017-20241115T150951Z-001/val2017\\000000406997.jpg\n",
      "Attempting to load image from: C:/Users/megha/Downloads/val2017-20241115T150951Z-001/val2017\\000000338986.jpg\n",
      "Attempting to load image from: C:/Users/megha/Downloads/val2017-20241115T150951Z-001/val2017\\000000410712.jpg\n",
      "Attempting to load image from: C:/Users/megha/Downloads/val2017-20241115T150951Z-001/val2017\\000000017115.jpg\n"
     ]
    }
   ],
   "source": [
    "# Augmentation and Preprocessing Pipeline\n",
    "transform_pipeline = A.Compose([\n",
    "    A.Resize(416, 416),  # Resize images to 416x416\n",
    "    A.RandomBrightnessContrast(p=0.2),  # Adjust brightness and contrast\n",
    "    A.GaussianBlur(p=0.2),  # Apply Gaussian blur\n",
    "    A.HorizontalFlip(p=0.5),  # Apply horizontal flip with 50% probability\n",
    "    A.Rotate(limit=20, p=0.5),  # Apply random rotation with a limit of 20 degrees\n",
    "    A.ColorJitter(p=0.3),  # Randomly change image color\n",
    "    A.ToGray(p=0.1),  # Randomly convert some images to grayscale\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),  # Normalize based on ImageNet values\n",
    "    ToTensorV2()  # Convert the image to PyTorch tensor\n",
    "], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['labels']))\n",
    "\n",
    "# Load COCO dataset (images, annotations, category_mapping) from JSON\n",
    "annotations_json_path = \"C:/Users/megha/Downloads/instances_val2017.json-20241115T150718Z-001/instances_val2017.json/instances_val2017.json\"\n",
    " #  your file path\n",
    "image_directory =  \"C:/Users/megha/Downloads/val2017-20241115T150951Z-001/val2017\"  #  your image directory path\n",
    "\n",
    "# Load annotations (COCO format)\n",
    "with open(annotations_json_path, 'r') as f:\n",
    "    coco_data = json.load(f)\n",
    "\n",
    "# Extract images, annotations, and categories\n",
    "image_list = coco_data['images']  # List of image data\n",
    "annotation_list = coco_data['annotations']  # List of annotations\n",
    "category_list = coco_data['categories']  # List of categories\n",
    "\n",
    "# Create a mapping of category IDs to category names\n",
    "category_id_mapping = {category['id']: category['name'] for category in category_list}\n",
    "\n",
    "# Initialize the custom dataset\n",
    "custom_dataset = CocoDataset(\n",
    "    image_metadata=image_list,\n",
    "    annotation_data=annotation_list,  \n",
    "    category_mapping=category_id_mapping,\n",
    "    image_dir=image_directory,\n",
    "    transform=transform_pipeline\n",
    ")\n",
    "\n",
    "# Improved collate_fn to handle dynamic batching and padding for images of varying sizes\n",
    "def dynamic_collate_fn(batch):\n",
    "    img_batch, target_batch = zip(*batch)\n",
    "\n",
    "    # Find the maximum height and width of images in the batch\n",
    "    max_img_height = max([img.shape[1] for img in img_batch])  # Maximum height\n",
    "    max_img_width = max([img.shape[2] for img in img_batch])   # Maximum width\n",
    "\n",
    "    padded_images = []\n",
    "    for img in img_batch:\n",
    "        # Create a tensor of zeros (padding) and copy the image into the top-left corner\n",
    "        padded_img = torch.zeros((3, max_img_height, max_img_width), dtype=torch.float32)\n",
    "        padded_img[:, :img.shape[1], :img.shape[2]] = img\n",
    "        padded_images.append(padded_img)\n",
    "\n",
    "    # Stack the padded images into a single tensor\n",
    "    img_batch = torch.stack(padded_images, dim=0)\n",
    "\n",
    "    return img_batch, target_batch\n",
    "\n",
    "# Set up DataLoader with the dynamic collate function\n",
    "data_loader = DataLoader(custom_dataset, batch_size=8, shuffle=True, collate_fn=dynamic_collate_fn)\n",
    "\n",
    "# Test the data loading process\n",
    "for batch_idx, (images, targets) in enumerate(data_loader):\n",
    "    if batch_idx >= 5:  # Limit the number of batches to test\n",
    "        break\n",
    "    print(f\"Batch {batch_idx} - Image batch size: {images.size()}\")\n",
    "    print(f\"Batch {batch_idx} - Number of targets: {len(targets)}\")\n",
    "    print(f\"Sample target: {targets[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
