{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "UyVzPVX_NR5Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43546e23-5f71-4efd-e764-aff2099646dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: albumentations in /usr/local/lib/python3.10/dist-packages (1.4.20)\n",
            "Requirement already satisfied: numpy>=1.24.4 in /usr/local/lib/python3.10/dist-packages (from albumentations) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from albumentations) (1.13.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from albumentations) (6.0.2)\n",
            "Requirement already satisfied: pydantic>=2.7.0 in /usr/local/lib/python3.10/dist-packages (from albumentations) (2.9.2)\n",
            "Requirement already satisfied: albucore==0.0.19 in /usr/local/lib/python3.10/dist-packages (from albumentations) (0.0.19)\n",
            "Requirement already satisfied: eval-type-backport in /usr/local/lib/python3.10/dist-packages (from albumentations) (0.2.0)\n",
            "Requirement already satisfied: opencv-python-headless>=4.9.0.80 in /usr/local/lib/python3.10/dist-packages (from albumentations) (4.10.0.84)\n",
            "Requirement already satisfied: stringzilla>=3.10.4 in /usr/local/lib/python3.10/dist-packages (from albucore==0.0.19->albumentations) (3.10.10)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.7.0->albumentations) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.7.0->albumentations) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.7.0->albumentations) (4.12.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (11.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: albumentations in /usr/local/lib/python3.10/dist-packages (1.4.20)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\n",
            "Requirement already satisfied: numpy>=1.24.4 in /usr/local/lib/python3.10/dist-packages (from albumentations) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from albumentations) (1.13.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from albumentations) (6.0.2)\n",
            "Requirement already satisfied: pydantic>=2.7.0 in /usr/local/lib/python3.10/dist-packages (from albumentations) (2.9.2)\n",
            "Requirement already satisfied: albucore==0.0.19 in /usr/local/lib/python3.10/dist-packages (from albumentations) (0.0.19)\n",
            "Requirement already satisfied: eval-type-backport in /usr/local/lib/python3.10/dist-packages (from albumentations) (0.2.0)\n",
            "Requirement already satisfied: opencv-python-headless>=4.9.0.80 in /usr/local/lib/python3.10/dist-packages (from albumentations) (4.10.0.84)\n",
            "Requirement already satisfied: stringzilla>=3.10.4 in /usr/local/lib/python3.10/dist-packages (from albucore==0.0.19->albumentations) (3.10.10)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (11.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.7.0->albumentations) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.7.0->albumentations) (2.23.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install albumentations\n",
        "!pip install torch torchvision\n",
        "!pip install albumentations torch torchvision\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "xms_Pk7cO5dU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9102568-682d-4494-a6c2-986b9a46bc2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "import json"
      ],
      "metadata": {
        "id": "Ijv8AAEWP-eC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "json_path = '/content/drive/MyDrive/Object Recognition Dataset/instances_val2017.json/instances_val2017.json'\n",
        "images_dir = '/content/drive/MyDrive/Object Recognition Dataset/val2017'"
      ],
      "metadata": {
        "id": "4FKGAlBgQA6s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pycocotools import mask as coco_mask  # Required for RLE decoding\n",
        "# Dataset class\n",
        "class CocoDataset(Dataset):\n",
        "    def __init__(self, images, annotations, category_mapping, img_dir, transform=None, image_size=(416, 416)):\n",
        "        self.images = images\n",
        "        self.annotations = annotations\n",
        "        self.category_mapping = category_mapping\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "        self.image_size = image_size\n",
        "        self.image_id_to_annotations = self._group_annotations_by_image()\n",
        "\n",
        "    def _group_annotations_by_image(self):\n",
        "        image_id_to_annotations = {}\n",
        "        for ann in self.annotations:\n",
        "            image_id = ann['image_id']\n",
        "            if image_id not in image_id_to_annotations:\n",
        "                image_id_to_annotations[image_id] = []\n",
        "            image_id_to_annotations[image_id].append(ann)\n",
        "        return image_id_to_annotations\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_info = self.images[idx]\n",
        "        img_path = os.path.join(self.img_dir, image_info['file_name'])\n",
        "        image = cv2.imread(img_path)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        # Get annotations\n",
        "        image_id = image_info['id']\n",
        "        annotations = self.image_id_to_annotations.get(image_id, [])\n",
        "        boxes = []\n",
        "        labels = []\n",
        "        masks = []\n",
        "\n",
        "        # Process masks and boxes\n",
        "        for ann in annotations:\n",
        "            if 'segmentation' in ann:\n",
        "                segmentation = ann['segmentation']\n",
        "                try:\n",
        "                    if isinstance(segmentation, dict):  # RLE format\n",
        "                        if isinstance(segmentation['counts'], list):\n",
        "                            segmentation = coco_mask.frPyObjects(segmentation, *segmentation['size'])\n",
        "                        mask = coco_mask.decode(segmentation)\n",
        "                    elif isinstance(segmentation, list):  # Polygon format\n",
        "                        mask = np.zeros((image.shape[0], image.shape[1]), dtype=np.uint8)\n",
        "                        if all(isinstance(poly, list) for poly in segmentation):\n",
        "                            for poly in segmentation:\n",
        "                                poly_array = np.array(poly).reshape(-1, 2).astype(np.int32)\n",
        "                                cv2.fillPoly(mask, [poly_array], 1)\n",
        "                        else:\n",
        "                            raise ValueError(f\"Invalid polygon format: {segmentation}\")\n",
        "                    else:\n",
        "                        raise ValueError(f\"Unknown segmentation format: {type(segmentation)}\")\n",
        "                    masks.append(mask)\n",
        "                except Exception as e:\n",
        "                    print(f\"Skipping invalid mask for annotation ID {ann['id']}: {e}\")\n",
        "                    continue\n",
        "\n",
        "            # Assuming bounding boxes are available in 'bbox' field (COCO format)\n",
        "            if 'bbox' in ann:\n",
        "                bbox = ann['bbox']  # COCO bbox format: [x_min, y_min, width, height]\n",
        "                boxes.append(bbox)\n",
        "                labels.append(ann['category_id'])  # Category label\n",
        "\n",
        "        # If there are no boxes, make sure we have empty tensors\n",
        "        if len(boxes) == 0:\n",
        "            boxes = torch.zeros((0, 4), dtype=torch.float32)  # No boxes\n",
        "            labels = torch.zeros((0,), dtype=torch.int64)  # No labels\n",
        "        else:\n",
        "            boxes = torch.tensor(boxes, dtype=torch.float32)\n",
        "            labels = torch.tensor(labels, dtype=torch.int64)\n",
        "\n",
        "        # Debugging: print labels to check for invalid ones\n",
        "        print(f\"Labels before transformation: {labels}\")\n",
        "\n",
        "        # Resize the image and masks to the same size\n",
        "        target_size = (416, 416)  # Change to your desired size\n",
        "        image = cv2.resize(image, target_size)\n",
        "        masks = [cv2.resize(mask, target_size, interpolation=cv2.INTER_NEAREST) for mask in masks]\n",
        "\n",
        "        # Debug: check image and mask sizes\n",
        "        print(f\"Image shape after resize: {image.shape}\")\n",
        "        print(f\"Mask shapes after resize: {[mask.shape for mask in masks]}\")\n",
        "\n",
        "        # Convert labels to NumPy arrays if required by albumentations\n",
        "        labels = labels.numpy()  # Convert to NumPy array\n",
        "        masks = np.array(masks)  # Convert masks to NumPy array\n",
        "\n",
        "        # Apply Albumentations transformations\n",
        "        if self.transform:\n",
        "            transformed = self.transform(image=image, masks=masks, bboxes=boxes, labels=labels)\n",
        "            image = transformed['image']\n",
        "            boxes = transformed['bboxes']\n",
        "            labels = transformed['labels']\n",
        "            masks = transformed['masks']\n",
        "\n",
        "        # Convert masks back to tensor\n",
        "        if len(masks) > 0:\n",
        "            masks = torch.as_tensor(np.stack(masks, axis=0), dtype=torch.uint8)  # Shape: (num_masks, height, width)\n",
        "        else:\n",
        "            masks = torch.zeros((0, *target_size), dtype=torch.uint8)  # Empty mask tensor\n",
        "\n",
        "        target = {\"boxes\": boxes, \"labels\": labels, \"masks\": masks}\n",
        "\n",
        "        return image, target\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jH3Qswx3m72e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n"
      ],
      "metadata": {
        "id": "cAHTTOyPnFYA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = A.Compose(\n",
        "    [\n",
        "        A.Resize(416, 416),  # Ensures all images and masks are resized consistently\n",
        "        A.RandomBrightnessContrast(p=0.2),\n",
        "        A.GaussianBlur(p=0.2),\n",
        "        A.HorizontalFlip(p=0.5),\n",
        "        A.Rotate(limit=20, p=0.5),\n",
        "        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
        "        ToTensorV2()\n",
        "    ],\n",
        "    bbox_params=A.BboxParams(format='pascal_voc', label_fields=['labels']),\n",
        "    additional_targets={'masks': 'masks'},\n",
        "    #is_check_shapes=False  # Disable shape check for edge cases\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "# Load COCO annotations\n",
        "annotations_file = '/content/drive/MyDrive/Object Recognition Dataset/instances_val2017.json/instances_val2017.json'\n",
        "img_dir = '/content/drive/MyDrive/Object Recognition Dataset/val2017'\n",
        "\n",
        "with open(annotations_file, 'r') as f:\n",
        "    coco_data = json.load(f)\n",
        "\n",
        "images = coco_data['images']\n",
        "annotations = coco_data['annotations']\n",
        "categories = coco_data['categories']\n",
        "category_mapping = {category['id']: category['name'] for category in categories}\n",
        "\n",
        "# Initialize dataset\n",
        "dataset = CocoDataset(images=images, annotations=annotations, category_mapping=category_mapping, img_dir=img_dir, transform=transform)\n",
        "\n",
        "# Define DataLoader\n",
        "data_loader = DataLoader(dataset, batch_size=4, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))\n"
      ],
      "metadata": {
        "id": "W6OAJENcnGGa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
        "# Mask R-CNN Model\n",
        "def create_custom_mask_rcnn(num_classes):\n",
        "    weights = torchvision.models.detection.MaskRCNN_ResNet50_FPN_Weights.DEFAULT\n",
        "    model = torchvision.models.detection.maskrcnn_resnet50_fpn(weights=weights)\n",
        "\n",
        "    # Update box predictor\n",
        "    in_features_box = model.roi_heads.box_predictor.cls_score.in_features\n",
        "    model.roi_heads.box_predictor = MaskRCNNPredictor(\n",
        "        in_features_box, 256, num_classes\n",
        "    )\n",
        "\n",
        "    # Update mask predictor\n",
        "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
        "    hidden_layer = 256\n",
        "    model.roi_heads.mask_predictor = MaskRCNNPredictor(\n",
        "        in_features_mask, hidden_layer, num_classes\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "# Move model to device\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "num_classes = 91  # Number of classes (including background)\n",
        "model = create_custom_mask_rcnn(num_classes)\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "sGVck67RnZPo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Optimizer and Scheduler\n",
        "params = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = torch.optim.Adam(params, lr=1e-4)\n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
        "\n",
        "# Training Loop\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for images, targets in data_loader:\n",
        "        images = [image.to(device) for image in images]\n",
        "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss_dict = model(images, targets)\n",
        "        losses = sum(loss for loss in loss_dict.values())\n",
        "        losses.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += losses.item()\n",
        "\n",
        "    lr_scheduler.step()\n",
        "    epoch_loss = running_loss / len(data_loader)\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}\")\n",
        "\n",
        "    # Save model checkpoint\n",
        "    torch.save(model.state_dict(), f\"mask_rcnn_epoch_{epoch+1}.pth\")\n",
        "\n",
        "print(\"Training completed!\")"
      ],
      "metadata": {
        "id": "YIIUNOpmGW4D",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation Mode\n",
        "model.eval()\n",
        "test_image, test_target = dataset[0]  # Get one sample\n",
        "test_image = test_image.to(device).unsqueeze(0)\n",
        "\n",
        "# Prediction\n",
        "with torch.no_grad():\n",
        "    predictions = model(test_image)\n",
        "\n",
        "# Visualization\n",
        "pred_image = test_image[0].permute(1, 2, 0).cpu().numpy()\n",
        "pred_image = (pred_image * 255).astype(np.uint8)\n",
        "\n",
        "for box, label, mask in zip(predictions[0]['boxes'], predictions[0]['labels'], predictions[0]['masks']):\n",
        "    x1, y1, x2, y2 = map(int, box)\n",
        "    color = (0, 255, 0)\n",
        "    cv2.rectangle(pred_image, (x1, y1), (x2, y2), color, 2)\n",
        "    text = category_mapping.get(label.item(), \"Unknown\")\n",
        "    cv2.putText(pred_image, text, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1)\n",
        "    mask = mask[0].cpu().numpy() > 0.5\n",
        "    pred_image[mask] = pred_image[mask] * 0.5 + np.array([0, 255, 0], dtype=np.uint8) * 0.5\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.imshow(pred_image)\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "yNuUwIRHGaD3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}