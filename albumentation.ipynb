{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W0mYMJxk9Uu6",
        "outputId": "f07e9983-941f-491b-8889-80355a2a1756"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U albumentations"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xHSReOseX_6G",
        "outputId": "d36652f7-91d4-40ff-f571-51969a29e4fe"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: albumentations in /usr/local/lib/python3.10/dist-packages (1.4.20)\n",
            "Collecting albumentations\n",
            "  Downloading albumentations-1.4.21-py3-none-any.whl.metadata (31 kB)\n",
            "Requirement already satisfied: numpy>=1.24.4 in /usr/local/lib/python3.10/dist-packages (from albumentations) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from albumentations) (1.13.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from albumentations) (6.0.2)\n",
            "Requirement already satisfied: pydantic>=2.7.0 in /usr/local/lib/python3.10/dist-packages (from albumentations) (2.9.2)\n",
            "Collecting albucore==0.0.20 (from albumentations)\n",
            "  Downloading albucore-0.0.20-py3-none-any.whl.metadata (5.3 kB)\n",
            "Requirement already satisfied: eval-type-backport in /usr/local/lib/python3.10/dist-packages (from albumentations) (0.2.0)\n",
            "Requirement already satisfied: opencv-python-headless>=4.9.0.80 in /usr/local/lib/python3.10/dist-packages (from albumentations) (4.10.0.84)\n",
            "Requirement already satisfied: stringzilla>=3.10.4 in /usr/local/lib/python3.10/dist-packages (from albucore==0.0.20->albumentations) (3.10.10)\n",
            "Collecting simsimd>=5.9.2 (from albucore==0.0.20->albumentations)\n",
            "  Downloading simsimd-6.0.5-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.7/57.7 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.7.0->albumentations) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.7.0->albumentations) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.7.0->albumentations) (4.12.2)\n",
            "Downloading albumentations-1.4.21-py3-none-any.whl (227 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.9/227.9 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading albucore-0.0.20-py3-none-any.whl (12 kB)\n",
            "Downloading simsimd-6.0.5-cp310-cp310-manylinux_2_28_x86_64.whl (605 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m605.1/605.1 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: simsimd, albucore, albumentations\n",
            "  Attempting uninstall: albucore\n",
            "    Found existing installation: albucore 0.0.19\n",
            "    Uninstalling albucore-0.0.19:\n",
            "      Successfully uninstalled albucore-0.0.19\n",
            "  Attempting uninstall: albumentations\n",
            "    Found existing installation: albumentations 1.4.20\n",
            "    Uninstalling albumentations-1.4.20:\n",
            "      Successfully uninstalled albumentations-1.4.20\n",
            "Successfully installed albucore-0.0.20 albumentations-1.4.21 simsimd-6.0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "import cv2\n",
        "import numpy as np\n",
        "import os"
      ],
      "metadata": {
        "id": "-WplEfn7ge3c"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CocoDataset(torch.utils.data.Dataset):  # Inherit from Dataset\n",
        "    def __init__(self, images, annotations, category_mapping, img_dir, transform=None):\n",
        "        self.images = images\n",
        "        self.annotations = annotations\n",
        "        self.category_mapping = category_mapping\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "        self.image_id_to_annotations = self._group_annotations_by_image()\n",
        "\n",
        "    def _group_annotations_by_image(self):\n",
        "        image_id_to_annotations = {}\n",
        "        for ann in self.annotations:\n",
        "            image_id = ann['image_id']\n",
        "            if image_id not in image_id_to_annotations:\n",
        "                image_id_to_annotations[image_id] = []\n",
        "            image_id_to_annotations[image_id].append(ann)\n",
        "        return image_id_to_annotations\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_info = self.images[idx]\n",
        "        img_path = os.path.join(self.img_dir, image_info['file_name'])\n",
        "        image = cv2.imread(img_path)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert to RGB\n",
        "\n",
        "        # Get annotations\n",
        "        image_id = image_info['id']\n",
        "        annotations = self.image_id_to_annotations.get(image_id, [])\n",
        "\n",
        "        boxes = []\n",
        "        labels = []\n",
        "        for ann in annotations:\n",
        "            x, y, width, height = ann['bbox']\n",
        "            boxes.append([x, y, x + width, y + height])\n",
        "            labels.append(ann['category_id'])\n",
        "\n",
        "        # Convert boxes and labels to numpy arrays for Albumentations\n",
        "        boxes = np.array(boxes)\n",
        "        labels = np.array(labels)\n",
        "\n",
        "        # Apply transformations\n",
        "        if self.transform:\n",
        "            transformed = self.transform(image=image, bboxes=boxes, labels=labels)\n",
        "            image = transformed['image']\n",
        "            boxes = transformed['bboxes']\n",
        "            labels = transformed['labels']\n",
        "\n",
        "        # Convert to PyTorch tensors\n",
        "        boxes = torch.tensor(boxes, dtype=torch.float32)\n",
        "        labels = torch.tensor(labels, dtype=torch.int64)\n",
        "        target = {\"boxes\": boxes, \"labels\": labels}\n",
        "\n",
        "        return image, target"
      ],
      "metadata": {
        "id": "Ifneu_9YgqHq"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json"
      ],
      "metadata": {
        "id": "nnjZXwbXlnnN"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "from torch.utils.data import DataLoader\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "annotation_file = '/content/drive/MyDrive/Colab Notebooks/instances_val2017.json'\n",
        "img_dir = '/content/drive/MyDrive/Colab Notebooks/val2017/'\n",
        "\n",
        "# Load annotations\n",
        "with open(annotation_file, 'r') as f:\n",
        "    coco_data = json.load(f)\n",
        "\n",
        "images = coco_data['images']\n",
        "annotations = coco_data['annotations']\n",
        "categories = coco_data['categories']\n",
        "category_mapping = {cat['id']: cat['name'] for cat in categories}\n",
        "print(f\"Loaded {len(annotations)} annotations and {len(categories)} categories.\")\n",
        "\n",
        "# Augmentation and Preprocessing Pipeline\n",
        "transform = A.Compose([\n",
        "    A.Resize(416, 416),\n",
        "    A.RandomBrightnessContrast(p=0.2),\n",
        "    A.GaussianBlur(p=0.2),\n",
        "    A.HorizontalFlip(p=0.5),\n",
        "    A.Rotate(limit=20, p=0.5),\n",
        "    A.ColorJitter(p=0.3),\n",
        "    A.ToGray(p=0.1),\n",
        "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
        "    ToTensorV2()\n",
        "], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['labels']))\n",
        "\n",
        "# Initialize Dataset and DataLoader\n",
        "dataset = CocoDataset(images, annotations, category_mapping, img_dir, transform=transform)\n",
        "data_loader = DataLoader(dataset, batch_size=16, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))\n",
        "\n",
        "# Test DataLoader\n",
        "for images_batch, targets_batch in data_loader:\n",
        "    print(f\"Images batch size: {len(images_batch)}\")\n",
        "    print(f\"Target batch size: {len(targets_batch)}\")\n",
        "    print(\"Sample target:\", targets_batch[0])\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yIRxnmuv6W00",
        "outputId": "4372790d-dcc8-49c1-ebd3-bd4c7e1aa522"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 36781 annotations and 80 categories.\n",
            "Images batch size: 16\n",
            "Target batch size: 16\n",
            "Sample target: {'boxes': tensor([[  0.0000,  29.7090,  21.5380, 284.8102],\n",
            "        [  0.0000,   5.3859,  10.0085,  75.0518],\n",
            "        [228.9655,  87.8621, 338.6541, 248.2668],\n",
            "        [112.6221,  59.0153, 233.2758, 242.0753],\n",
            "        [  0.0000,  48.1224, 145.0379, 303.2236],\n",
            "        [ 66.8842, 182.9871, 244.2605, 416.0000],\n",
            "        [ 80.4507, 173.2080, 204.8816, 284.9275],\n",
            "        [270.2061, 193.3108, 362.2168, 265.6737],\n",
            "        [214.7188, 156.0565, 416.0000, 416.0000],\n",
            "        [ 46.7985, 285.1402, 103.2587, 317.2188],\n",
            "        [  6.0799, 287.2697, 103.3571, 393.9852],\n",
            "        [ 33.9609,  15.2013,  75.8408,  84.8672],\n",
            "        [352.5294, 104.7198, 416.0000, 197.7580],\n",
            "        [  0.0000, 323.1266,  55.7543, 390.1275],\n",
            "        [174.2113,  44.9631, 200.7932,  73.0731],\n",
            "        [183.3839,  49.8341, 258.7166, 176.8713],\n",
            "        [354.2028, 245.4057, 390.5701, 303.6892],\n",
            "        [115.1521,  77.5213, 145.1959, 126.4052],\n",
            "        [109.7884, 302.4429, 193.3943, 398.2441],\n",
            "        [100.3701, 230.4442, 149.0595, 269.2175],\n",
            "        [384.9996, 269.4224, 407.9999, 296.3614],\n",
            "        [180.8500, 360.3510, 229.9479, 416.0000],\n",
            "        [172.4729, 327.5160, 234.0430, 416.0000],\n",
            "        [137.6175, 235.7516, 179.3427, 310.0713],\n",
            "        [275.1387, 230.5246, 290.0993, 244.7740],\n",
            "        [ 71.1913,  40.4681, 141.1801, 179.4733],\n",
            "        [384.5134, 181.3723, 416.0000, 416.0000],\n",
            "        [405.8037, 272.5242, 416.0000, 299.4632],\n",
            "        [ 34.9369, 397.2593, 212.3132, 416.0000],\n",
            "        [  0.0000, 394.5215,  87.3662, 416.0000],\n",
            "        [  0.0000, 387.1931,  46.2022, 416.0000],\n",
            "        [172.8136, 414.2512, 221.9116, 416.0000],\n",
            "        [159.2873, 415.9525, 220.8574, 416.0000]]), 'labels': tensor([ 1,  1,  1,  1,  1, 73, 73, 73, 73, 74, 76,  1,  1,  1,  1,  1, 47, 73,\n",
            "        76, 76, 76, 47, 47, 47, 76,  1, 73, 76, 73, 76,  1, 47, 47])}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "from torch.utils.data import DataLoader\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "import torch\n",
        "from torch.nn.functional import pad\n",
        "\n",
        "annotation_file = '/content/drive/MyDrive/Colab Notebooks/instances_val2017.json'\n",
        "img_dir = '/content/drive/MyDrive/Colab Notebooks/val2017/'\n",
        "\n",
        "# Load annotations\n",
        "with open(annotation_file, 'r') as f:\n",
        "    coco_data = json.load(f)\n",
        "\n",
        "images = coco_data['images']\n",
        "annotations = coco_data['annotations']\n",
        "categories = coco_data['categories']\n",
        "category_mapping = {cat['id']: cat['name'] for cat in categories}\n",
        "print(f\"Loaded {len(annotations)} annotations and {len(categories)} categories.\")\n",
        "\n",
        "# Augmentation and Preprocessing Pipeline\n",
        "transform = A.Compose([\n",
        "    A.Resize(416, 416),  # Resize the image to 416x416\n",
        "    A.RandomBrightnessContrast(p=0.2),\n",
        "    A.GaussianBlur(p=0.2),\n",
        "    A.HorizontalFlip(p=0.5),\n",
        "    A.Rotate(limit=20, p=0.5),\n",
        "    A.ColorJitter(p=0.3),\n",
        "    A.ToGray(p=0.1),\n",
        "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
        "    ToTensorV2()  # Don't include PadIfNeeded here\n",
        "], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['labels']))\n",
        "\n",
        "# Collate function for dynamic batching\n",
        "def collate_fn(batch):\n",
        "    images, targets = zip(*batch)  # Unpack batch into images and targets\n",
        "\n",
        "    # Pad images to the same size manually\n",
        "    max_height = max(image.shape[1] for image in images)\n",
        "    max_width = max(image.shape[2] for image in images)\n",
        "\n",
        "    padded_images = []\n",
        "    for image in images:\n",
        "        _, h, w = image.shape\n",
        "        pad_height = max_height - h\n",
        "        pad_width = max_width - w\n",
        "        # Manually pad (left, right, top, bottom) to match the largest image in the batch\n",
        "        padded_image = pad(image, (0, pad_width, 0, pad_height))  # Pad left, right, top, bottom\n",
        "        padded_images.append(padded_image)\n",
        "\n",
        "    # Stack padded images into a tensor\n",
        "    padded_images = torch.stack(padded_images)\n",
        "\n",
        "    # Print padded images and targets\n",
        "    print(f\"Padded Images: {padded_images.shape}\")\n",
        "\n",
        "    return padded_images, targets\n",
        "\n",
        "# Initialize Dataset and DataLoader\n",
        "dataset = CocoDataset(images, annotations, category_mapping, img_dir, transform=transform)\n",
        "data_loader = DataLoader(dataset, batch_size=16, shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "# Test DataLoader\n",
        "for images_batch, targets_batch in data_loader:\n",
        "    print(f\"Images Batch Size: {images_batch.shape}\")\n",
        "    print(f\"Target Batch Size: {len(targets_batch)}\")\n",
        "    print(\"Sample Target:\", targets_batch[0])\n",
        "    break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xi7azlUF1BhD",
        "outputId": "37c8b4c9-ff30-4c57-f002-4e37ef3d35fd"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 36781 annotations and 80 categories.\n",
            "Padded Images: torch.Size([16, 3, 416, 416])\n",
            "Images Batch Size: torch.Size([16, 3, 416, 416])\n",
            "Target Batch Size: 16\n",
            "Sample Target: {'boxes': tensor([[159.3368, 149.9225, 365.7877, 395.6160],\n",
            "        [ 53.3006, 112.9960, 125.1118, 192.1465],\n",
            "        [188.1742, 109.7980, 263.5024, 167.4985],\n",
            "        [113.8398, 119.4830, 125.9593, 125.9375],\n",
            "        [133.1297, 119.9770, 148.9806, 126.4640],\n",
            "        [123.9037, 148.2650, 136.8806, 172.1330],\n",
            "        [106.9909, 140.4715, 121.0882, 148.8630],\n",
            "        [393.5925, 147.5305, 407.5923, 173.0950],\n",
            "        [146.3209, 145.2100, 191.0385, 171.8990],\n",
            "        [ 32.1791, 142.8700,  58.3374, 174.8175],\n",
            "        [ 35.5207, 145.3335,  49.3452, 152.0675],\n",
            "        [  0.0000, 115.1735,  37.5082, 168.8635],\n",
            "        [ 95.3683, 113.6850, 159.0737, 168.9480],\n",
            "        [188.3399, 147.0365, 201.5018, 165.5485]]), 'labels': tensor([19,  1,  1,  1,  1,  1,  1,  1,  3,  3,  3,  6,  6, 84])}\n"
          ]
        }
      ]
    }
  ]
}