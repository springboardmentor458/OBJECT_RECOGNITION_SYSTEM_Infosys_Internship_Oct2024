{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5qx8OsPcEQhC",
    "outputId": "0ce36026-3559-4c2b-9ab6-bc2a9dc6b975"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: albumentations in /usr/local/lib/python3.10/dist-packages (1.4.21)\n",
      "Requirement already satisfied: numpy>=1.24.4 in /usr/local/lib/python3.10/dist-packages (from albumentations) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from albumentations) (1.13.1)\n",
      "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from albumentations) (6.0.2)\n",
      "Requirement already satisfied: pydantic>=2.7.0 in /usr/local/lib/python3.10/dist-packages (from albumentations) (2.9.2)\n",
      "Requirement already satisfied: albucore==0.0.20 in /usr/local/lib/python3.10/dist-packages (from albumentations) (0.0.20)\n",
      "Requirement already satisfied: eval-type-backport in /usr/local/lib/python3.10/dist-packages (from albumentations) (0.2.0)\n",
      "Requirement already satisfied: opencv-python-headless>=4.9.0.80 in /usr/local/lib/python3.10/dist-packages (from albumentations) (4.10.0.84)\n",
      "Requirement already satisfied: stringzilla>=3.10.4 in /usr/local/lib/python3.10/dist-packages (from albucore==0.0.20->albumentations) (3.10.8)\n",
      "Requirement already satisfied: simsimd>=5.9.2 in /usr/local/lib/python3.10/dist-packages (from albucore==0.0.20->albumentations) (6.0.5)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.7.0->albumentations) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.7.0->albumentations) (2.23.4)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.7.0->albumentations) (4.12.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U albumentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9D6__5oJNWj4",
    "outputId": "da59fd79-9e7d-4fcb-b9c1-0a2a08458789"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "f1tcm619Ntxx"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "SE_nySCqNnLE"
   },
   "outputs": [],
   "source": [
    "json_path = '/content/drive/MyDrive/Colab_Notebooks/instances_val2017.json/instances_val2017.json'\n",
    "images_dir = '/content/drive/MyDrive/Colab_Notebooks/val2017/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1Eodu7wu4n3e"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "rhQgpoVoQ6Yw"
   },
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mGXrtxPaIJlP",
    "outputId": "bd2519f8-6f5e-47b3-eb93-fa6abfdcbbe3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch of images size: torch.Size([8, 3, 416, 416])\n",
      "Batch of targets length: 8\n",
      "Sample target: {'boxes': tensor([[  0.0000,   0.0000, 293.3548,  25.5915],\n",
      "        [ 39.5258,   0.0000, 395.4639, 416.0000],\n",
      "        [ 41.4800,  81.0893, 136.4931, 240.2232],\n",
      "        [330.4786, 123.0851, 382.6027, 162.3719],\n",
      "        [138.6789, 380.1142, 416.0000, 416.0000]]), 'labels': tensor([ 1,  1, 43, 37,  1])}\n",
      "Batch of images size: torch.Size([8, 3, 416, 416])\n",
      "Batch of targets length: 8\n",
      "Sample target: {'boxes': tensor([[ 75.9525,  16.8285, 245.3913, 401.9795],\n",
      "        [262.9494, 173.8685, 300.1944, 220.4995]]), 'labels': tensor([25, 25])}\n",
      "Batch of images size: torch.Size([8, 3, 416, 416])\n",
      "Batch of targets length: 8\n",
      "Sample target: {'boxes': tensor([[ 13.7363,   1.9305, 375.9309, 411.3916],\n",
      "        [368.2349,   3.3629, 416.0000, 129.2714],\n",
      "        [207.8752, 225.8232, 273.5533, 282.0206],\n",
      "        [139.1770, 195.9559, 206.0947, 271.2096],\n",
      "        [  0.7238,   3.2383, 142.1888, 408.4398]]), 'labels': tensor([51,  1, 52, 52,  1])}\n",
      "Batch of images size: torch.Size([8, 3, 416, 416])\n",
      "Batch of targets length: 8\n",
      "Sample target: {'boxes': tensor([[164.6580, 226.3757, 227.6365, 349.5758],\n",
      "        [122.0765, 239.1564, 180.0240, 341.9345],\n",
      "        [ 62.6145, 279.2242, 140.3220, 341.9055],\n",
      "        [212.5955, 274.3661, 314.0150, 341.2558]]), 'labels': tensor([ 1,  1, 15, 15])}\n",
      "Batch of images size: torch.Size([8, 3, 416, 416])\n",
      "Batch of targets length: 8\n",
      "Sample target: {'boxes': tensor([[178.5133,   0.0000, 416.0000,  34.2255],\n",
      "        [  0.0000,   0.0000,   4.8013, 115.1180],\n",
      "        [ 54.6826, 115.2387, 267.8685, 351.7616],\n",
      "        [ 43.2841,   0.0000, 196.0590, 146.5379],\n",
      "        [149.1794,   0.0000, 416.0000, 212.7855],\n",
      "        [ 84.0919, 191.8559, 413.4669, 416.0000],\n",
      "        [379.5305, 240.3906, 416.0000, 416.0000],\n",
      "        [ 50.1555, 398.4324, 379.5305, 416.0000]]), 'labels': tensor([ 1,  1, 77,  1,  1,  1,  1,  1])}\n"
     ]
    }
   ],
   "source": [
    "# Augmentation and Preprocessing Pipeline\n",
    "transform = A.Compose([\n",
    "    A.Resize(416, 416),  # Resizing\n",
    "    A.RandomBrightnessContrast(p=0.2),  # Brightness and Contrast Adjustment\n",
    "    A.GaussianBlur(p=0.2),  # Blurring for image quality improvement\n",
    "    A.HorizontalFlip(p=0.5),  # Horizontal Flip\n",
    "    A.Rotate(limit=20, p=0.5),  # Random Rotation\n",
    "    A.ColorJitter(p=0.3),  # Random color adjustments\n",
    "    A.ToGray(p=0.1),  # Randomly convert some images to grayscale\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),  # Normalization\n",
    "    ToTensorV2()  # Convert image to PyTorch tensor\n",
    "], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['labels']))\n",
    "\n",
    "# Load COCO dataset (images, annotations, category_mapping) from JSON\n",
    "annotations_file = '/content/drive/MyDrive/Colab_Notebooks/instances_val2017.json/instances_val2017.json'\n",
    "img_dir = '/content/drive/MyDrive/Colab_Notebooks/val2017/'\n",
    "\n",
    "# Load annotations (COCO-style)\n",
    "with open(annotations_file, 'r') as f:\n",
    "    coco_data = json.load(f)\n",
    "\n",
    "# Extract images, annotations, and category mapping\n",
    "images = coco_data['images']  # List of images\n",
    "annotations = coco_data['annotations']  # List of annotations\n",
    "categories = coco_data['categories']  # List of categories\n",
    "\n",
    "# Create category mapping (optional)\n",
    "category_mapping = {category['id']: category['name'] for category in categories}\n",
    "\n",
    "# Initialize dataset\n",
    "dataset = CocoDataset(images=images, annotations=annotations, category_mapping=category_mapping, img_dir=img_dir, transform=transform)\n",
    "\n",
    "# Improved collate_fn handling dynamic batching and padding for varying image sizes\n",
    "def collate_fn(batch):\n",
    "    images, targets = zip(*batch)\n",
    "\n",
    "    # Handle variable image sizes and padding if necessary\n",
    "    max_height = max([img.shape[1] for img in images])  # Find the max height\n",
    "    max_width = max([img.shape[2] for img in images])   # Find the max width\n",
    "\n",
    "    padded_images = []\n",
    "    for img in images:\n",
    "        # Pad images to the maximum width and height\n",
    "        padded_img = torch.zeros((3, max_height, max_width), dtype=torch.float32)\n",
    "        padded_img[:, :img.shape[1], :img.shape[2]] = img\n",
    "        padded_images.append(padded_img)\n",
    "\n",
    "    images = torch.stack(padded_images, dim=0)  # Stack images to create a batch\n",
    "\n",
    "    return images, targets\n",
    "\n",
    "# Update DataLoader with the modified collate function\n",
    "data_loader = DataLoader(dataset, batch_size=8, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "# Test the new data loading\n",
    "for idx, (images, targets) in enumerate(data_loader):\n",
    "    if idx >= 5:  # Set the number of batches you want to test\n",
    "        break\n",
    "    print(f\"Batch of images size: {images.size()}\")\n",
    "    print(f\"Batch of targets length: {len(targets)}\")\n",
    "    print(\"Sample target:\", targets[0])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
