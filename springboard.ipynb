{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/springboardmentor458/OBJECT_RECOGNITION_SYSTEM_Infosys_Internship_Oct2024/blob/Harsha-Kota/springboard.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "J-i8C6UjAFKH",
        "outputId": "27ee65c8-5853-4730-d312-75de2934847a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.10/dist-packages (4.10.0.84)\n",
            "Requirement already satisfied: albumentations in /usr/local/lib/python3.10/dist-packages (1.4.20)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.0+cu121)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python-headless) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from albumentations) (1.13.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from albumentations) (6.0.2)\n",
            "Requirement already satisfied: pydantic>=2.7.0 in /usr/local/lib/python3.10/dist-packages (from albumentations) (2.9.2)\n",
            "Requirement already satisfied: albucore==0.0.19 in /usr/local/lib/python3.10/dist-packages (from albumentations) (0.0.19)\n",
            "Requirement already satisfied: eval-type-backport in /usr/local/lib/python3.10/dist-packages (from albumentations) (0.2.0)\n",
            "Requirement already satisfied: stringzilla>=3.10.4 in /usr/local/lib/python3.10/dist-packages (from albucore==0.0.19->albumentations) (3.10.8)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.7.0->albumentations) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.7.0->albumentations) (2.23.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install opencv-python-headless albumentations torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uLqLDVhjDwWn",
        "outputId": "f08491d3-15ff-4a07-c75c-773a272d3f45"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6hkweBLvCR3c"
      },
      "outputs": [],
      "source": [
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "import os\n",
        "import json\n",
        "import urllib.request"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uKl4KWnHC15B"
      },
      "source": [
        "Use this if you need all those images in folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tymPmUYxCEVj"
      },
      "outputs": [],
      "source": [
        "#Remove comments to run\n",
        "\"\"\"\n",
        "# Cell 1\n",
        "import json\n",
        "import cv2\n",
        "import urllib.request\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "json_filename = '/content/drive/MyDrive/Colab Notebooks/instances_val2017.json'\n",
        "with open(json_filename, 'r') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "output_dir = '/content/output_images'\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "def download_and_save(image_url, annotations, filename):\n",
        "    resp = urllib.request.urlopen(image_url)\n",
        "    img_array = np.asarray(bytearray(resp.read()), dtype=np.uint8)\n",
        "    img = cv2.imdecode(img_array, cv2.IMREAD_COLOR)\n",
        "    for ann in annotations:\n",
        "        x, y, w, h = ann['bbox']\n",
        "        cv2.rectangle(img, (int(x), int(y)), (int(x+w), int(y+h)), (255, 0, 0), 2)\n",
        "    output_path = os.path.join(output_dir, filename)\n",
        "    cv2.imwrite(output_path, img)\n",
        "\n",
        "for img_data in data['images']:\n",
        "    img_id = img_data['id']\n",
        "    img_url = img_data['coco_url']\n",
        "    filename = img_data['file_name']\n",
        "    annotations = [ann for ann in data.get('annotations', []) if ann['image_id'] == img_id]\n",
        "    download_and_save(img_url, annotations, filename)\n",
        "\n",
        "print(f\"All images saved in {output_dir}\")\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "juF4bqxVCw7H"
      },
      "source": [
        "Use This for Direct Representations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wStW0cFZCuVJ"
      },
      "outputs": [],
      "source": [
        "# Cell 1\n",
        "import json\n",
        "import cv2\n",
        "import urllib.request\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.gridspec import GridSpec\n",
        "\n",
        "json_filename = '/content/drive/MyDrive/Colab Notebooks/instances_val2017.json'\n",
        "with open(json_filename, 'r') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "def download_and_display(image_url, annotations, image=None):\n",
        "    resp = urllib.request.urlopen(image_url)\n",
        "    img_array = np.asarray(bytearray(resp.read()), dtype=np.uint8)\n",
        "    img = cv2.imdecode(img_array, cv2.IMREAD_COLOR)\n",
        "\n",
        "    if image is not None:\n",
        "        img = image\n",
        "\n",
        "    for ann in annotations:\n",
        "        x, y, w, h = ann['bbox']\n",
        "        cv2.rectangle(img, (int(x), int(y)), (int(x+w), int(y+h)), (255, 0, 0), 2)\n",
        "\n",
        "    plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# Get the user input for the number of images to load\n",
        "num_images = int(input(\"Enter the number of images to load (default is all): \") or len(data['images']))\n",
        "\n",
        "original_images = []\n",
        "original_annotations = []\n",
        "\n",
        "for i, img_data in enumerate(data['images']):\n",
        "    if i >= num_images:\n",
        "        break\n",
        "    img_id = img_data['id']\n",
        "    img_url = img_data['coco_url']\n",
        "    annotations = [ann for ann in data.get('annotations', []) if ann['image_id'] == img_id]\n",
        "    original_images.append(img_url)\n",
        "    original_annotations.append(annotations)\n",
        "    download_and_display(img_url, annotations)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bBg9h2vIX5b"
      },
      "source": [
        "Image Processing Pipeline with Annotations and Visualization(Test 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "dAVQNJlnvH6R"
      },
      "outputs": [],
      "source": [
        "# Cell 2\n",
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "def process_images(original_images, original_annotations):\n",
        "    processed_images = []\n",
        "    processed_annotations = []\n",
        "\n",
        "    for img_url, annotations in zip(original_images, original_annotations):\n",
        "        resp = urllib.request.urlopen(img_url)\n",
        "        img_array = np.asarray(bytearray(resp.read()), dtype=np.uint8)\n",
        "        img = cv2.imdecode(img_array, cv2.IMREAD_COLOR)\n",
        "\n",
        "        # Filter images\n",
        "        gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "        binary_img = cv2.threshold(gray_img, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)[1]\n",
        "\n",
        "        # Resize images\n",
        "        resized_img = cv2.resize(img, (224, 224))\n",
        "\n",
        "        # Test for correct orientation\n",
        "        height, width, _ = img.shape\n",
        "        if height > width:\n",
        "            rotated_img = cv2.rotate(img, cv2.ROTATE_90_CLOCKWISE)\n",
        "        else:\n",
        "            rotated_img = img\n",
        "\n",
        "        # Fix blurry images\n",
        "        sharpened_img = cv2.GaussianBlur(rotated_img, (0, 0), sigmaX=1, sigmaY=1)\n",
        "        sharpened_img = cv2.addWeighted(rotated_img, 1.5, sharpened_img, -0.5, 0)\n",
        "\n",
        "        # Apply filters (e.g., black and white)\n",
        "        bw_img = cv2.cvtColor(sharpened_img, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "        processed_images.extend([img, binary_img, resized_img, rotated_img, sharpened_img, bw_img])\n",
        "        processed_annotations.extend([annotations] * 6)\n",
        "\n",
        "    return processed_images, processed_annotations\n",
        "\n",
        "# Call the processing function with the data from the previous cell\n",
        "processed_images, processed_annotations = process_images(original_images, original_annotations)\n",
        "\n",
        "# Display the processed images\n",
        "for i, (img, annotations) in enumerate(zip(processed_images, processed_annotations)):\n",
        "    download_and_display(original_images[i % len(original_images)], annotations, image=img)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oHLCeVLfILTD"
      },
      "source": [
        "Image Processing Pipeline with Annotations and Visualization(Test 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "DS_P_m3hwj5J"
      },
      "outputs": [],
      "source": [
        "# Cell 2\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.gridspec import GridSpec\n",
        "\n",
        "def process_and_display_images(original_images, original_annotations):\n",
        "    def draw_annotations(image, annotations):\n",
        "        # Create a copy of the image to avoid modifying the original\n",
        "        annotated_img = image.copy()\n",
        "        for ann in annotations:\n",
        "            x, y, w, h = ann['bbox']\n",
        "            cv2.rectangle(annotated_img,\n",
        "                         (int(x), int(y)),\n",
        "                         (int(x+w), int(y+h)),\n",
        "                         (255, 0, 0), 2)\n",
        "        return annotated_img\n",
        "\n",
        "    def create_image_grid(images, titles, num_cols=3):\n",
        "        num_images = len(images)\n",
        "        num_rows = (num_images + num_cols - 1) // num_cols\n",
        "\n",
        "        fig = plt.figure(figsize=(15, 5 * num_rows))\n",
        "        gs = GridSpec(num_rows, num_cols, figure=fig)\n",
        "\n",
        "        for idx, (img, title) in enumerate(zip(images, titles)):\n",
        "            row = idx // num_cols\n",
        "            col = idx % num_cols\n",
        "            ax = fig.add_subplot(gs[row, col])\n",
        "\n",
        "            if len(img.shape) == 2:  # If grayscale\n",
        "                # Convert grayscale to BGR for annotation\n",
        "                img_color = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)\n",
        "                ax.imshow(cv2.cvtColor(img_color, cv2.COLOR_BGR2RGB))\n",
        "            else:  # If color\n",
        "                ax.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
        "\n",
        "            ax.set_title(title)\n",
        "            ax.axis('off')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    for img_url, annotations in zip(original_images, original_annotations):\n",
        "        # Download and process each image\n",
        "        resp = urllib.request.urlopen(img_url)\n",
        "        img_array = np.asarray(bytearray(resp.read()), dtype=np.uint8)\n",
        "        img = cv2.imdecode(img_array, cv2.IMREAD_COLOR)\n",
        "\n",
        "        # Process images with different techniques\n",
        "        processed_images = []\n",
        "        titles = []\n",
        "\n",
        "        # 1. Original Image with annotations\n",
        "        annotated_original = draw_annotations(img, annotations)\n",
        "        processed_images.append(annotated_original)\n",
        "        titles.append('Original Image with Annotations')\n",
        "\n",
        "        # 2. Binary Image with annotations\n",
        "        gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "        binary_img = cv2.threshold(gray_img, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)[1]\n",
        "        binary_img_color = cv2.cvtColor(binary_img, cv2.COLOR_GRAY2BGR)\n",
        "        annotated_binary = draw_annotations(binary_img_color, annotations)\n",
        "        processed_images.append(annotated_binary)\n",
        "        titles.append('Binary Image with Annotations')\n",
        "\n",
        "        # 3. Resized Image with annotations\n",
        "        resized_img = cv2.resize(img, (224, 224))\n",
        "        # Scale annotations for resized image\n",
        "        scale_x = 224.0 / img.shape[1]\n",
        "        scale_y = 224.0 / img.shape[0]\n",
        "        scaled_annotations = []\n",
        "        for ann in annotations:\n",
        "            scaled_ann = ann.copy()\n",
        "            x, y, w, h = ann['bbox']\n",
        "            scaled_ann['bbox'] = [x * scale_x, y * scale_y, w * scale_x, h * scale_y]\n",
        "            scaled_annotations.append(scaled_ann)\n",
        "        annotated_resized = draw_annotations(resized_img, scaled_annotations)\n",
        "        processed_images.append(annotated_resized)\n",
        "        titles.append('Resized Image (224x224) with Annotations')\n",
        "\n",
        "        # 4. Rotated Image with annotations (portrait orientation images will rotate.)\n",
        "        height, width, _ = img.shape\n",
        "        if height > width:              # If you want to rotate all images regardless of their orientation remove the line or adjust\n",
        "            rotated_img = cv2.rotate(img, cv2.ROTATE_90_CLOCKWISE)\n",
        "            # Adjust annotations for rotation\n",
        "            rotated_annotations = []\n",
        "            for ann in annotations:\n",
        "                rotated_ann = ann.copy()\n",
        "                x, y, w, h = ann['bbox']\n",
        "                rotated_ann['bbox'] = [y, width - x - w, h, w]\n",
        "                rotated_annotations.append(rotated_ann)\n",
        "            rotation_status = \"Rotated 90°\"\n",
        "            annotated_rotated = draw_annotations(rotated_img, rotated_annotations)\n",
        "        else:\n",
        "            rotated_img = img\n",
        "            rotation_status = \"No Rotation Needed\"\n",
        "            annotated_rotated = draw_annotations(rotated_img, annotations)\n",
        "        processed_images.append(annotated_rotated)\n",
        "        titles.append(f'Orientation Check\\n({rotation_status})')\n",
        "\n",
        "        # 5. Sharpened Image with annotations\n",
        "        sharpened_img = cv2.GaussianBlur(rotated_img, (0, 0), sigmaX=1, sigmaY=1)\n",
        "        sharpened_img = cv2.addWeighted(rotated_img, 1.5, sharpened_img, -0.5, 0)\n",
        "        annotated_sharpened = draw_annotations(sharpened_img,\n",
        "                                             rotated_annotations if height > width else annotations)\n",
        "        processed_images.append(annotated_sharpened)\n",
        "        titles.append('Sharpened Image with Annotations')\n",
        "\n",
        "        # 6. Black and White Image with annotations\n",
        "        bw_img = cv2.cvtColor(sharpened_img, cv2.COLOR_BGR2GRAY)\n",
        "        bw_img_color = cv2.cvtColor(bw_img, cv2.COLOR_GRAY2BGR)\n",
        "        annotated_bw = draw_annotations(bw_img_color,\n",
        "                                      rotated_annotations if height > width else annotations)\n",
        "        processed_images.append(annotated_bw)\n",
        "        titles.append('Black & White Image with Annotations')\n",
        "\n",
        "        # Display grid for current image\n",
        "        create_image_grid(processed_images, titles)\n",
        "\n",
        "# Usage:\n",
        "processed_images, processed_annotations = process_and_display_images(original_images, original_annotations)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-IWOtULJJy9"
      },
      "source": [
        "Custom PyTorch Dataset Class for COCO-style Object Detection"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import json\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "import multiprocessing\n",
        "\n",
        "class CocoDataset(Dataset):\n",
        "    def __init__(self, images, annotations, category_mapping, img_dir, transform=None):\n",
        "        self.images = images\n",
        "        self.annotations = annotations\n",
        "        self.category_mapping = category_mapping\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "        self.image_id_to_annotations = self._group_annotations_by_image()\n",
        "        print(\"COCO Dataset Class initialized successfully!\")\n",
        "\n",
        "    def _group_annotations_by_image(self):\n",
        "        image_id_to_annotations = {}\n",
        "        for ann in self.annotations:\n",
        "            image_id = ann['image_id']\n",
        "            if image_id not in image_id_to_annotations:\n",
        "                image_id_to_annotations[image_id] = []\n",
        "            image_id_to_annotations[image_id].append(ann)\n",
        "        return image_id_to_annotations\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_info = self.images[idx]\n",
        "        img_path = os.path.join(self.img_dir, image_info['file_name'])\n",
        "\n",
        "        # Load image\n",
        "        try:\n",
        "            image = cv2.imread(img_path)\n",
        "            if image is not None:\n",
        "                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "                print(f\"Successfully loaded image: {img_path}\")\n",
        "            else:\n",
        "                raise FileNotFoundError(f\"Image not found or failed to load: {img_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading image at {img_path}: {e}\")\n",
        "            return None, None\n",
        "\n",
        "        # Get annotations\n",
        "        image_id = image_info['id']\n",
        "        annotations = self.image_id_to_annotations.get(image_id, [])\n",
        "        if not annotations:\n",
        "            print(f\"No annotations found for image ID {image_id}\")\n",
        "            return None, None\n",
        "\n",
        "        boxes = []\n",
        "        labels = []\n",
        "        for ann in annotations:\n",
        "            x, y, width, height = ann['bbox']\n",
        "            boxes.append([x, y, x + width, y + height])\n",
        "            labels.append(self.category_mapping.get(ann['category_id'], -1))  # Default to -1 if not found\n",
        "\n",
        "        # Convert boxes and labels to numpy arrays\n",
        "        try:\n",
        "            boxes = np.array(boxes, dtype=np.float32)\n",
        "            labels = np.array(labels, dtype=np.int64)\n",
        "        except ValueError as e:\n",
        "            print(f\"Error processing labels for image ID {image_id}: {e}\")\n",
        "            return None, None\n",
        "\n",
        "        # Apply transformations\n",
        "        if self.transform:\n",
        "            transformed = self.transform(image=image, bboxes=boxes, labels=labels)\n",
        "            image = transformed['image']\n",
        "            boxes = transformed['bboxes']\n",
        "            labels = transformed['labels']\n",
        "        else:\n",
        "            print(f\"No transformations applied to image ID {image_id}\")\n",
        "\n",
        "        # Convert to PyTorch tensors\n",
        "        if len(boxes) == 0 or len(labels) == 0:\n",
        "            print(f\"Warning: No valid boxes or labels for image ID {image_id}\")\n",
        "            return None, None\n",
        "\n",
        "        boxes = torch.tensor(boxes, dtype=torch.float32)\n",
        "        labels = torch.tensor(labels, dtype=torch.int64)\n",
        "        target = {\"boxes\": boxes, \"labels\": labels}\n",
        "\n",
        "        return image, target\n",
        "\n",
        "\n",
        "# Load JSON file\n",
        "with open('/content/drive/MyDrive/Colab Notebooks/instances_val2017.json', 'r') as f:\n",
        "    try:\n",
        "        coco_data = json.load(f)\n",
        "        print(\"JSON file loaded successfully!\")\n",
        "    except json.JSONDecodeError as e:\n",
        "        print(f\"Error loading JSON file: {e}\")\n",
        "        coco_data = {}\n",
        "\n",
        "images = coco_data.get('images', [])\n",
        "annotations = coco_data.get('annotations', [])\n",
        "categories = coco_data.get('categories', [])\n",
        "\n",
        "if not images or not annotations or not categories:\n",
        "    print(\"Warning: Missing data in COCO JSON.\")\n",
        "else:\n",
        "    print(f\"Loaded {len(images)} images, {len(annotations)} annotations, and {len(categories)} categories.\")\n",
        "\n",
        "# Map category IDs to numeric labels\n",
        "category_mapping = {category['id']: idx for idx, category in enumerate(categories)}\n",
        "\n",
        "# Augmentation and Preprocessing Pipeline\n",
        "transform = A.Compose([\n",
        "    A.Resize(height=416, width=416),\n",
        "    A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.2),\n",
        "    A.GaussianBlur(blur_limit=(3, 7), p=0.2),\n",
        "    A.HorizontalFlip(p=0.5),\n",
        "    A.Rotate(limit=20, p=0.5),\n",
        "    A.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1, p=0.3),\n",
        "    A.ToGray(p=0.1),\n",
        "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
        "    ToTensorV2()\n",
        "], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['labels']))\n",
        "\n",
        "coco_dataset = CocoDataset(images, annotations, category_mapping, '/content/drive/MyDrive/Colab Notebooks/val2017', transform=transform)\n",
        "\n",
        "# Adjust num_workers based on CPU count\n",
        "num_workers = min(2, multiprocessing.cpu_count())\n",
        "data_loader = DataLoader(coco_dataset, batch_size=16, shuffle=True, collate_fn=lambda x: tuple(zip(*x)), num_workers=num_workers)\n",
        "\n",
        "# Process DataLoader\n",
        "num_valid_samples = 0\n",
        "num_skipped_samples = 0\n",
        "for images, targets in data_loader:\n",
        "    if images is not None and targets is not None:\n",
        "        print(f\"Images batch size: {len(images)}\")\n",
        "        print(f\"Target batch size: {len(targets)}\")\n",
        "        print(\"Sample target:\", targets[0])\n",
        "        num_valid_samples += len(images)\n",
        "    else:\n",
        "        num_skipped_samples += 1\n",
        "        print(f\"Skipping invalid sample ({num_skipped_samples})\")\n",
        "\n",
        "print(f\"Total number of valid samples: {num_valid_samples}\")\n",
        "print(f\"Total number of skipped samples: {num_skipped_samples}\")\n"
      ],
      "metadata": {
        "id": "Hq_Iu0z3_wIY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#improved\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import json\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "import multiprocessing\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "class CocoDataset(Dataset):\n",
        "    def __init__(self, images, annotations, category_mapping, img_dir, transform=None, input_size=(416, 416)):\n",
        "        self.images = images\n",
        "        self.annotations = annotations\n",
        "        self.category_mapping = category_mapping\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "        self.input_size = input_size\n",
        "        self.image_id_to_annotations = self._group_annotations_by_image()\n",
        "        print(f\"COCO Dataset initialized with {len(images)} images.\")\n",
        "\n",
        "    def _group_annotations_by_image(self):\n",
        "        image_id_to_annotations = {}\n",
        "        for ann in self.annotations:\n",
        "            image_id = ann['image_id']\n",
        "            if image_id not in image_id_to_annotations:\n",
        "                image_id_to_annotations[image_id] = []\n",
        "            image_id_to_annotations[image_id].append(ann)\n",
        "        return image_id_to_annotations\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_info = self.images[idx]\n",
        "        img_path = os.path.join(self.img_dir, image_info['file_name'])\n",
        "\n",
        "        # Load and preprocess image\n",
        "        try:\n",
        "            image = cv2.imread(img_path)\n",
        "            if image is None:\n",
        "                raise FileNotFoundError(f\"Image not found: {img_path}\")\n",
        "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading image {img_path}: {e}\")\n",
        "            return None, None\n",
        "\n",
        "        # Get original image size\n",
        "        original_size = image.shape[:2]\n",
        "\n",
        "        # Resize image dynamically to the input size\n",
        "        image = cv2.resize(image, (self.input_size[1], self.input_size[0]))\n",
        "\n",
        "        # Get annotations for the image\n",
        "        image_id = image_info['id']\n",
        "        annotations = self.image_id_to_annotations.get(image_id, [])\n",
        "        if not annotations:\n",
        "            return None, None\n",
        "\n",
        "        boxes = []\n",
        "        labels = []\n",
        "        for ann in annotations:\n",
        "            x, y, width, height = ann['bbox']\n",
        "            # Convert bbox to format (xmin, ymin, xmax, ymax)\n",
        "            boxes.append([x, y, x + width, y + height])\n",
        "            labels.append(self.category_mapping.get(ann['category_id'], -1))\n",
        "\n",
        "        try:\n",
        "            boxes = np.array(boxes, dtype=np.float32)\n",
        "            labels = np.array(labels, dtype=np.int64)\n",
        "        except ValueError as e:\n",
        "            print(f\"Invalid data for image ID {image_id}: {e}\")\n",
        "            return None, None\n",
        "\n",
        "        # Normalize bounding boxes relative to the resized image\n",
        "        image_height, image_width = image.shape[:2]\n",
        "        boxes[:, [0, 2]] /= image_width  # Normalize x-min and x-max\n",
        "        boxes[:, [1, 3]] /= image_height  # Normalize y-min and y-max\n",
        "\n",
        "        # Apply augmentations\n",
        "        if self.transform:\n",
        "            transformed = self.transform(image=image, bboxes=boxes, labels=labels)\n",
        "            image = transformed['image']\n",
        "            boxes = transformed['bboxes']\n",
        "            labels = transformed['labels']\n",
        "\n",
        "        # Convert to PyTorch tensors\n",
        "        if len(boxes) == 0 or len(labels) == 0:\n",
        "            return None, None\n",
        "        boxes = torch.tensor(boxes, dtype=torch.float32)\n",
        "        labels = torch.tensor(labels, dtype=torch.int64)\n",
        "        target = {\"boxes\": boxes, \"labels\": labels, \"original_size\": torch.tensor(original_size)}\n",
        "\n",
        "        return image, target\n",
        "\n",
        "\n",
        "# Load and parse JSON\n",
        "def load_coco_json(json_path):\n",
        "    try:\n",
        "        with open(json_path, 'r') as f:\n",
        "            coco_data = json.load(f)\n",
        "        print(\"JSON file loaded successfully!\")\n",
        "        return coco_data\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading JSON file: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "# Automatically detect GPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "\n",
        "# Augmentation and Preprocessing\n",
        "def get_augmentation_pipeline(mode='train'):\n",
        "    if mode == 'train':\n",
        "        return A.Compose([\n",
        "            A.Resize(416, 416),\n",
        "            A.HorizontalFlip(p=0.5),\n",
        "            A.VerticalFlip(p=0.2),  # New\n",
        "            A.Rotate(limit=10, p=0.5),\n",
        "            A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1, p=0.5),  # New\n",
        "            A.RandomResizedCrop(416, 416, scale=(0.8, 1.0), ratio=(0.75, 1.33), p=0.5),  # New\n",
        "            A.GaussianBlur(blur_limit=(3, 5), p=0.2),  # New\n",
        "            A.GridDistortion(p=0.3),  # New\n",
        "            A.Perspective(scale=(0.05, 0.1), p=0.3),  # New\n",
        "            A.CoarseDropout(max_holes=8, max_height=32, max_width=32, fill_value=0, p=0.5),  # New\n",
        "            A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
        "            ToTensorV2(),\n",
        "        ], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['labels']))\n",
        "    elif mode == 'val':\n",
        "        return A.Compose([\n",
        "            A.Resize(416, 416),\n",
        "            A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
        "            ToTensorV2(),\n",
        "        ], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['labels']))\n",
        "\n",
        "\n",
        "# Load data\n",
        "json_path = '/content/drive/MyDrive/Colab Notebooks/instances_val2017.json'\n",
        "coco_data = load_coco_json(json_path)\n",
        "if coco_data:\n",
        "    images = coco_data.get('images', [])\n",
        "    annotations = coco_data.get('annotations', [])\n",
        "    categories = coco_data.get('categories', [])\n",
        "    category_mapping = {category['id']: idx for idx, category in enumerate(categories)}\n",
        "\n",
        "    coco_dataset = CocoDataset(\n",
        "        images,\n",
        "        annotations,\n",
        "        category_mapping,\n",
        "        '/content/drive/MyDrive/Colab Notebooks/val2017',\n",
        "        transform=get_augmentation_pipeline('train')\n",
        "    )\n",
        "\n",
        "    # Dynamic Batch Sizing\n",
        "    memory_limit = torch.cuda.get_device_properties(0).total_memory // (1024 ** 3) if torch.cuda.is_available() else 8  # Default 8GB for CPU\n",
        "    batch_size = min(max(4, memory_limit // 2), 16)  # Dynamic batch size between 4 and 16\n",
        "    print(f\"Using dynamic batch size: {batch_size}\")\n",
        "\n",
        "    # DataLoader\n",
        "    num_workers = min(2, multiprocessing.cpu_count())\n",
        "    data_loader = DataLoader(\n",
        "        coco_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        collate_fn=lambda x: tuple(zip(*x)),\n",
        "        num_workers=num_workers\n",
        "    )\n",
        "\n",
        "    # Process DataLoader with progress tracking\n",
        "    valid_samples = 0\n",
        "    skipped_samples = 0\n",
        "    for batch in tqdm(data_loader, desc=\"Processing DataLoader\"):\n",
        "        images, targets = batch\n",
        "        if images is not None and targets is not None:\n",
        "            valid_samples += len(images)\n",
        "            print(f\"Processed batch: {len(images)} images\")\n",
        "        else:\n",
        "            skipped_samples += 1\n",
        "\n",
        "    print(f\"Valid samples: {valid_samples}\")\n",
        "    print(f\"Skipped samples: {skipped_samples}\")\n",
        "else:\n",
        "    print(\"Failed to load COCO data.\")\n"
      ],
      "metadata": {
        "id": "H7Hwr17BOxj4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#improvement testing\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import json\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "import multiprocessing\n",
        "from tqdm import tqdm\n",
        "import logging\n",
        "\n",
        "# Initialize logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "class CocoDataset(Dataset):\n",
        "    def __init__(self, images, annotations, category_mapping, img_dir, transform=None, input_size=(416, 416)):\n",
        "        self.images = images\n",
        "        self.annotations = annotations\n",
        "        self.category_mapping = category_mapping\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "        self.input_size = input_size\n",
        "        self.image_id_to_annotations = self._group_annotations_by_image()\n",
        "        logging.info(f\"COCO Dataset initialized with {len(images)} images.\")\n",
        "\n",
        "    def _group_annotations_by_image(self):\n",
        "        image_id_to_annotations = {}\n",
        "        for ann in self.annotations:\n",
        "            image_id = ann['image_id']\n",
        "            if image_id not in image_id_to_annotations:\n",
        "                image_id_to_annotations[image_id] = []\n",
        "            image_id_to_annotations[image_id].append(ann)\n",
        "        return image_id_to_annotations\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_info = self.images[idx]\n",
        "        img_path = os.path.join(self.img_dir, image_info['file_name'])\n",
        "\n",
        "        # Load and preprocess image\n",
        "        try:\n",
        "            image = cv2.imread(img_path)\n",
        "            if image is None:\n",
        "                raise FileNotFoundError(f\"Image not found: {img_path}\")\n",
        "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error loading image {img_path}: {e}\")\n",
        "            return None, None\n",
        "\n",
        "        # Get original image size\n",
        "        original_size = image.shape[:2]\n",
        "\n",
        "        # Resize image dynamically to the input size\n",
        "        image = cv2.resize(image, (self.input_size[1], self.input_size[0]))\n",
        "\n",
        "        # Get annotations for the image\n",
        "        image_id = image_info['id']\n",
        "        annotations = self.image_id_to_annotations.get(image_id, [])\n",
        "        if not annotations:\n",
        "            logging.warning(f\"No annotations found for image {img_path}\")\n",
        "            return None, None\n",
        "\n",
        "        boxes = []\n",
        "        labels = []\n",
        "        for ann in annotations:\n",
        "            try:\n",
        "                x, y, width, height = ann['bbox']\n",
        "                if width <= 0 or height <= 0:\n",
        "                    raise ValueError(f\"Invalid bbox dimensions: {ann['bbox']}\")\n",
        "                boxes.append([x, y, x + width, y + height])\n",
        "                labels.append(self.category_mapping.get(ann['category_id'], -1))\n",
        "            except KeyError as e:\n",
        "                logging.error(f\"Missing key in annotation for image {image_id}: {e}\")\n",
        "                continue\n",
        "            except ValueError as e:\n",
        "                logging.error(f\"Invalid value in annotation for image {image_id}: {e}\")\n",
        "                continue\n",
        "\n",
        "        # Normalize bounding boxes relative to the resized image\n",
        "        try:\n",
        "            boxes = np.array(boxes, dtype=np.float32)\n",
        "            labels = np.array(labels, dtype=np.int64)\n",
        "        except ValueError as e:\n",
        "            logging.error(f\"Invalid data for image ID {image_id}: {e}\")\n",
        "            return None, None\n",
        "\n",
        "        image_height, image_width = image.shape[:2]\n",
        "        boxes[:, [0, 2]] /= image_width  # Normalize x-min and x-max\n",
        "        boxes[:, [1, 3]] /= image_height  # Normalize y-min and y-max\n",
        "\n",
        "        # Apply augmentations\n",
        "        if self.transform:\n",
        "            transformed = self.transform(image=image, bboxes=boxes, labels=labels)\n",
        "            image = transformed['image']\n",
        "            boxes = transformed['bboxes']\n",
        "            labels = transformed['labels']\n",
        "\n",
        "        # Convert to PyTorch tensors\n",
        "        if len(boxes) == 0 or len(labels) == 0:\n",
        "            return None, None\n",
        "        boxes = torch.tensor(boxes, dtype=torch.float32)\n",
        "        labels = torch.tensor(labels, dtype=torch.int64)\n",
        "        target = {\"boxes\": boxes, \"labels\": labels, \"original_size\": torch.tensor(original_size)}\n",
        "\n",
        "        return image, target\n",
        "\n",
        "# Load and parse JSON\n",
        "def load_coco_json(json_path):\n",
        "    try:\n",
        "        with open(json_path, 'r') as f:\n",
        "            coco_data = json.load(f)\n",
        "        logging.info(\"JSON file loaded successfully!\")\n",
        "        return coco_data\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error loading JSON file: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "# Automatically detect GPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "logging.info(f\"Using device: {device}\")\n",
        "\n",
        "\n",
        "# Augmentation and Preprocessing\n",
        "def get_augmentation_pipeline(mode='train'):\n",
        "    if mode == 'train':\n",
        "        return A.Compose([\n",
        "            A.Resize(416, 416),\n",
        "            A.HorizontalFlip(p=0.5),\n",
        "            A.VerticalFlip(p=0.2),\n",
        "            A.Rotate(limit=10, p=0.5),\n",
        "            A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1, p=0.5),\n",
        "            A.RandomResizedCrop(416, 416, scale=(0.8, 1.0), ratio=(0.75, 1.33), p=0.5),\n",
        "            A.GaussianBlur(blur_limit=(3, 5), p=0.2),\n",
        "            A.GridDistortion(p=0.3),\n",
        "            A.Perspective(scale=(0.05, 0.1), p=0.3),\n",
        "            A.CoarseDropout(max_holes=8, max_height=32, max_width=32, fill_value=0, p=0.5),\n",
        "            A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
        "            ToTensorV2(),\n",
        "        ], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['labels']))\n",
        "    elif mode == 'val':\n",
        "        return A.Compose([\n",
        "            A.Resize(416, 416),\n",
        "            A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
        "            ToTensorV2(),\n",
        "        ], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['labels']))\n",
        "\n",
        "# Load data\n",
        "json_path = '/content/drive/MyDrive/Colab Notebooks/instances_val2017.json'\n",
        "coco_data = load_coco_json(json_path)\n",
        "if coco_data:\n",
        "    images = coco_data.get('images', [])\n",
        "    annotations = coco_data.get('annotations', [])\n",
        "    categories = coco_data.get('categories', [])\n",
        "    category_mapping = {category['id']: idx for idx, category in enumerate(categories)}\n",
        "\n",
        "    coco_dataset = CocoDataset(\n",
        "        images,\n",
        "        annotations,\n",
        "        category_mapping,\n",
        "        '/content/drive/MyDrive/Colab Notebooks/val2017',\n",
        "        transform=get_augmentation_pipeline('train')\n",
        "    )\n",
        "\n",
        "    # Dynamic Batch Sizing\n",
        "    if torch.cuda.is_available():\n",
        "        memory_limit = torch.cuda.get_device_properties(0).total_memory * 0.8  # 80% of available GPU memory\n",
        "        batch_size = 16  # Start with a default batch size\n",
        "    else:\n",
        "        memory_limit = 0  # Default to 0 for CPU\n",
        "        batch_size = 4  # Use smaller batch size for CPU\n",
        "\n",
        "    while batch_size > 1:\n",
        "        try:\n",
        "            data_loader = DataLoader(\n",
        "                coco_dataset, batch_size=batch_size, shuffle=True,\n",
        "                num_workers=multiprocessing.cpu_count(), collate_fn=lambda x: tuple(zip(*x))\n",
        "            )\n",
        "            break\n",
        "        except RuntimeError as e:\n",
        "            if 'out of memory' in str(e):\n",
        "                logging.warning(f\"Batch size {batch_size} too large, reducing.\")\n",
        "                batch_size -= 2\n",
        "            else:\n",
        "                logging.error(f\"Error in data loader: {e}\")\n",
        "                break\n",
        "\n",
        "    logging.info(f\"Final batch size: {batch_size}\")\n",
        "\n",
        "    # A simple example of loading a batch of data (to verify everything is working)\n",
        "    for images, targets in data_loader:\n",
        "        if images is not None:\n",
        "            logging.info(f\"Loaded batch with {len(images)} images.\")\n",
        "        else:\n",
        "            logging.warning(\"Empty batch loaded!\")\n",
        "        break  # Only load one batch for testing\n"
      ],
      "metadata": {
        "id": "JIrQ_dHRTF1e",
        "outputId": "afb5639d-8c97-417b-bd2c-495a8e0595ce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/albumentations/core/bbox_utils.py:478: RuntimeWarning: invalid value encountered in divide\n",
            "  & (clipped_box_areas / denormalized_box_areas >= min_visibility - epsilon)\n",
            "/usr/local/lib/python3.10/dist-packages/albumentations/augmentations/dropout/functional.py:211: RuntimeWarning: invalid value encountered in divide\n",
            "  visibility_ratio = 1 - (intersection_area / box_areas[i])\n",
            "/usr/local/lib/python3.10/dist-packages/albumentations/core/bbox_utils.py:478: RuntimeWarning: invalid value encountered in divide\n",
            "  & (clipped_box_areas / denormalized_box_areas >= min_visibility - epsilon)\n",
            "/usr/local/lib/python3.10/dist-packages/albumentations/augmentations/dropout/functional.py:211: RuntimeWarning: invalid value encountered in divide\n",
            "  visibility_ratio = 1 - (intersection_area / box_areas[i])\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}