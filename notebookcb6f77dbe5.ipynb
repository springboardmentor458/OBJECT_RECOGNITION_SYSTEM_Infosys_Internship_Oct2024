{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9953407,"sourceType":"datasetVersion","datasetId":6121409},{"sourceId":9973602,"sourceType":"datasetVersion","datasetId":6136319}],"dockerImageVersionId":30786,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install --upgrade albumentations\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T12:35:24.491095Z","iopub.execute_input":"2024-11-21T12:35:24.492204Z","iopub.status.idle":"2024-11-21T12:39:25.958092Z","shell.execute_reply.started":"2024-11-21T12:35:24.492149Z","shell.execute_reply":"2024-11-21T12:39:25.956417Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: albumentations in /opt/conda/lib/python3.10/site-packages (1.4.17)\n\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7f02e7c617e0>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/albumentations/\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7f02e7c61ae0>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/albumentations/\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7f02e7c61c90>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/albumentations/\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7f02e7c61e40>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/albumentations/\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7f02e7c61ff0>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/albumentations/\u001b[0m\u001b[33m\n\u001b[0mRequirement already satisfied: numpy>=1.24.4 in /opt/conda/lib/python3.10/site-packages (from albumentations) (1.26.4)\nRequirement already satisfied: scipy>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from albumentations) (1.14.1)\nRequirement already satisfied: scikit-image>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from albumentations) (0.23.2)\nRequirement already satisfied: PyYAML in /opt/conda/lib/python3.10/site-packages (from albumentations) (6.0.2)\nRequirement already satisfied: pydantic>=2.7.0 in /opt/conda/lib/python3.10/site-packages (from albumentations) (2.9.2)\nRequirement already satisfied: albucore==0.0.17 in /opt/conda/lib/python3.10/site-packages (from albumentations) (0.0.17)\nRequirement already satisfied: eval-type-backport in /opt/conda/lib/python3.10/site-packages (from albumentations) (0.2.0)\nRequirement already satisfied: opencv-python-headless>=4.9.0.80 in /opt/conda/lib/python3.10/site-packages (from albumentations) (4.10.0.84)\nRequirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from pydantic>=2.7.0->albumentations) (0.7.0)\nRequirement already satisfied: pydantic-core==2.23.4 in /opt/conda/lib/python3.10/site-packages (from pydantic>=2.7.0->albumentations) (2.23.4)\nRequirement already satisfied: typing-extensions>=4.6.1 in /opt/conda/lib/python3.10/site-packages (from pydantic>=2.7.0->albumentations) (4.12.2)\nRequirement already satisfied: networkx>=2.8 in /opt/conda/lib/python3.10/site-packages (from scikit-image>=0.21.0->albumentations) (3.3)\nRequirement already satisfied: pillow>=9.1 in /opt/conda/lib/python3.10/site-packages (from scikit-image>=0.21.0->albumentations) (10.3.0)\nRequirement already satisfied: imageio>=2.33 in /opt/conda/lib/python3.10/site-packages (from scikit-image>=0.21.0->albumentations) (2.34.1)\nRequirement already satisfied: tifffile>=2022.8.12 in /opt/conda/lib/python3.10/site-packages (from scikit-image>=0.21.0->albumentations) (2024.5.22)\nRequirement already satisfied: packaging>=21 in /opt/conda/lib/python3.10/site-packages (from scikit-image>=0.21.0->albumentations) (21.3)\nRequirement already satisfied: lazy-loader>=0.4 in /opt/conda/lib/python3.10/site-packages (from scikit-image>=0.21.0->albumentations) (0.4)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=21->scikit-image>=0.21.0->albumentations) (3.1.2)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nfrom torch.utils.data import Dataset, DataLoader\nimport cv2\nimport numpy as np\nimport torch\nimport os\nimport json\nimport urllib.request\n     ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T12:39:25.960722Z","iopub.execute_input":"2024-11-21T12:39:25.961093Z","iopub.status.idle":"2024-11-21T12:39:25.967369Z","shell.execute_reply.started":"2024-11-21T12:39:25.961059Z","shell.execute_reply":"2024-11-21T12:39:25.966127Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"import cv2\nimport numpy as np\nimport json\nfrom PIL import Image\n\n# Define the local file paths and annotations as lists\noriginal_images = [\"/kaggle/input/images/val2017-20241107T150103Z-001/val2017/000000000285.jpg\"]\noriginal_annotations = [\n    '/kaggle/input/images/instances_val2017.json'\n]\n\n# Define the processing function\ndef process_data(imgs, anns):\n    processed_imgs = []\n    processed_anns = []\n\n    for img_path, ann_path in zip(imgs, anns):\n        # Read the image from the local path\n        img = cv2.imread(img_path)\n\n        # Check if the image is loaded successfully\n        if img is None:\n            print(f\"Error loading image at {img_path}\")\n            continue\n\n        # Load and parse JSON annotations\n        try:\n            with open(ann_path, 'r') as f:\n                annotation_data = json.load(f)\n        except Exception as e:\n            print(f\"Error loading annotation JSON: {e}\")\n            continue\n\n        # Convert to grayscale\n        gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n\n        # Convert to binary using thresholding\n        _, binary_img = cv2.threshold(\n            gray_img, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU\n        )\n\n        # Resize image\n        resized_img = cv2.resize(img, (224, 224))\n\n        # Rotate if height > width\n        height, width, _ = img.shape\n        if height > width:\n            rotated_img = cv2.rotate(img, cv2.ROTATE_90_CLOCKWISE)\n        else:\n            rotated_img = img\n\n        # Sharpen the image\n        sharpened_img = cv2.GaussianBlur(rotated_img, (0, 0), sigmaX=1, sigmaY=1)\n        sharpened_img = cv2.addWeighted(rotated_img, 1.5, sharpened_img, -0.5, 0)\n\n        # Convert sharpened image to grayscale\n        bw_img = cv2.cvtColor(sharpened_img, cv2.COLOR_BGR2GRAY)\n\n        # Append all processed images and annotations\n        processed_imgs.extend([img, binary_img, resized_img, rotated_img, sharpened_img, bw_img])\n        processed_anns.extend([annotation_data] * 6)\n\n    return processed_imgs, processed_anns\n\n# Call the processing function\nprocessed_imgs, processed_anns = process_data(original_images, original_annotations)\n\n# Example placeholder for `download_and_display` function\ndef display_image_with_annotation(image, annotation):\n    \"\"\"\n    Displays the processed image with annotations.\n    \"\"\"\n    # Convert BGR to RGB for matplotlib display\n    img_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) if len(image.shape) == 3 else image\n\n    # Display the image\n    plt.imshow(img_rgb, cmap='gray' if len(image.shape) == 2 else None)\n    plt.title(f\"Annotation: {annotation}\")\n    plt.axis('off')\n    plt.show()\n\n# Display the processed images\nfor img, ann in zip(processed_imgs, processed_anns):\n    display_image_with_annotation(img, ann)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T12:53:54.107701Z","iopub.execute_input":"2024-11-21T12:53:54.108759Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/IPython/core/formatters.py:340\u001b[0m, in \u001b[0;36mBaseFormatter.__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    338\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 340\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mprinter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[38;5;66;03m# Finally look for special method names\u001b[39;00m\n\u001b[1;32m    342\u001b[0m method \u001b[38;5;241m=\u001b[39m get_real_method(obj, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_method)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/IPython/core/pylabtools.py:152\u001b[0m, in \u001b[0;36mprint_figure\u001b[0;34m(fig, fmt, bbox_inches, base64, **kwargs)\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend_bases\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FigureCanvasBase\n\u001b[1;32m    150\u001b[0m     FigureCanvasBase(fig)\n\u001b[0;32m--> 152\u001b[0m \u001b[43mfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcanvas\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprint_figure\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbytes_io\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    153\u001b[0m data \u001b[38;5;241m=\u001b[39m bytes_io\u001b[38;5;241m.\u001b[39mgetvalue()\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fmt \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msvg\u001b[39m\u001b[38;5;124m'\u001b[39m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/matplotlib/backend_bases.py:2366\u001b[0m, in \u001b[0;36mFigureCanvasBase.print_figure\u001b[0;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, pad_inches, bbox_extra_artists, backend, **kwargs)\u001b[0m\n\u001b[1;32m   2362\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   2363\u001b[0m     \u001b[38;5;66;03m# _get_renderer may change the figure dpi (as vector formats\u001b[39;00m\n\u001b[1;32m   2364\u001b[0m     \u001b[38;5;66;03m# force the figure dpi to 72), so we need to set it again here.\u001b[39;00m\n\u001b[1;32m   2365\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m cbook\u001b[38;5;241m.\u001b[39m_setattr_cm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfigure, dpi\u001b[38;5;241m=\u001b[39mdpi):\n\u001b[0;32m-> 2366\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mprint_method\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2367\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2368\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfacecolor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfacecolor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2369\u001b[0m \u001b[43m            \u001b[49m\u001b[43medgecolor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43medgecolor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2370\u001b[0m \u001b[43m            \u001b[49m\u001b[43morientation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morientation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2371\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbbox_inches_restore\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_bbox_inches_restore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2372\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2373\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   2374\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m bbox_inches \u001b[38;5;129;01mand\u001b[39;00m restore_bbox:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/matplotlib/backend_bases.py:2232\u001b[0m, in \u001b[0;36mFigureCanvasBase._switch_canvas_and_return_print_method.<locals>.<lambda>\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   2228\u001b[0m     optional_kws \u001b[38;5;241m=\u001b[39m {  \u001b[38;5;66;03m# Passed by print_figure for other renderers.\u001b[39;00m\n\u001b[1;32m   2229\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdpi\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfacecolor\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124medgecolor\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morientation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2230\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbbox_inches_restore\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[1;32m   2231\u001b[0m     skip \u001b[38;5;241m=\u001b[39m optional_kws \u001b[38;5;241m-\u001b[39m {\u001b[38;5;241m*\u001b[39minspect\u001b[38;5;241m.\u001b[39msignature(meth)\u001b[38;5;241m.\u001b[39mparameters}\n\u001b[0;32m-> 2232\u001b[0m     print_method \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mwraps(meth)(\u001b[38;5;28;01mlambda\u001b[39;00m \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: \u001b[43mmeth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2233\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mskip\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   2234\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# Let third-parties do as they see fit.\u001b[39;00m\n\u001b[1;32m   2235\u001b[0m     print_method \u001b[38;5;241m=\u001b[39m meth\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/matplotlib/backends/backend_agg.py:509\u001b[0m, in \u001b[0;36mFigureCanvasAgg.print_png\u001b[0;34m(self, filename_or_obj, metadata, pil_kwargs)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprint_png\u001b[39m(\u001b[38;5;28mself\u001b[39m, filename_or_obj, \u001b[38;5;241m*\u001b[39m, metadata\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, pil_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    463\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    464\u001b[0m \u001b[38;5;124;03m    Write the figure to a PNG file.\u001b[39;00m\n\u001b[1;32m    465\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    507\u001b[0m \u001b[38;5;124;03m        *metadata*, including the default 'Software' key.\u001b[39;00m\n\u001b[1;32m    508\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 509\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_print_pil\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename_or_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpng\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpil_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/matplotlib/backends/backend_agg.py:457\u001b[0m, in \u001b[0;36mFigureCanvasAgg._print_pil\u001b[0;34m(self, filename_or_obj, fmt, pil_kwargs, metadata)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_print_pil\u001b[39m(\u001b[38;5;28mself\u001b[39m, filename_or_obj, fmt, pil_kwargs, metadata\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    453\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    454\u001b[0m \u001b[38;5;124;03m    Draw the canvas, then save it using `.image.imsave` (to which\u001b[39;00m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;124;03m    *pil_kwargs* and *metadata* are forwarded).\u001b[39;00m\n\u001b[1;32m    456\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 457\u001b[0m     \u001b[43mFigureCanvasAgg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    458\u001b[0m     mpl\u001b[38;5;241m.\u001b[39mimage\u001b[38;5;241m.\u001b[39mimsave(\n\u001b[1;32m    459\u001b[0m         filename_or_obj, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer_rgba(), \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39mfmt, origin\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mupper\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    460\u001b[0m         dpi\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfigure\u001b[38;5;241m.\u001b[39mdpi, metadata\u001b[38;5;241m=\u001b[39mmetadata, pil_kwargs\u001b[38;5;241m=\u001b[39mpil_kwargs)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/matplotlib/backends/backend_agg.py:394\u001b[0m, in \u001b[0;36mFigureCanvasAgg.draw\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdraw\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    393\u001b[0m     \u001b[38;5;66;03m# docstring inherited\u001b[39;00m\n\u001b[0;32m--> 394\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrenderer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_renderer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    395\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrenderer\u001b[38;5;241m.\u001b[39mclear()\n\u001b[1;32m    396\u001b[0m     \u001b[38;5;66;03m# Acquire a lock on the shared font cache.\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/matplotlib/_api/deprecation.py:384\u001b[0m, in \u001b[0;36mdelete_parameter.<locals>.wrapper\u001b[0;34m(*inner_args, **inner_kwargs)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39minner_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minner_kwargs):\n\u001b[1;32m    381\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(inner_args) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m name_idx \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m inner_kwargs:\n\u001b[1;32m    382\u001b[0m         \u001b[38;5;66;03m# Early return in the simple, non-deprecated case (much faster than\u001b[39;00m\n\u001b[1;32m    383\u001b[0m         \u001b[38;5;66;03m# calling bind()).\u001b[39;00m\n\u001b[0;32m--> 384\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minner_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minner_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    385\u001b[0m     arguments \u001b[38;5;241m=\u001b[39m signature\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39minner_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minner_kwargs)\u001b[38;5;241m.\u001b[39marguments\n\u001b[1;32m    386\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_varargs \u001b[38;5;129;01mand\u001b[39;00m arguments\u001b[38;5;241m.\u001b[39mget(name):\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/matplotlib/backends/backend_agg.py:411\u001b[0m, in \u001b[0;36mFigureCanvasAgg.get_renderer\u001b[0;34m(self, cleared)\u001b[0m\n\u001b[1;32m    409\u001b[0m reuse_renderer \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lastKey \u001b[38;5;241m==\u001b[39m key)\n\u001b[1;32m    410\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m reuse_renderer:\n\u001b[0;32m--> 411\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrenderer \u001b[38;5;241m=\u001b[39m \u001b[43mRendererAgg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfigure\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdpi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    412\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lastKey \u001b[38;5;241m=\u001b[39m key\n\u001b[1;32m    413\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m cleared:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/matplotlib/backends/backend_agg.py:84\u001b[0m, in \u001b[0;36mRendererAgg.__init__\u001b[0;34m(self, width, height, dpi)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwidth \u001b[38;5;241m=\u001b[39m width\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheight \u001b[38;5;241m=\u001b[39m height\n\u001b[0;32m---> 84\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_renderer \u001b[38;5;241m=\u001b[39m \u001b[43m_RendererAgg\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mwidth\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mheight\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdpi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_filter_renderers \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_methods()\n","\u001b[0;31mValueError\u001b[0m: Image size of 12348609x411 pixels is too large. It must be less than 2^16 in each direction."],"ename":"ValueError","evalue":"Image size of 12348609x411 pixels is too large. It must be less than 2^16 in each direction.","output_type":"error"},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>"},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"import json\nimport cv2\nimport urllib.request\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Load COCO-style JSON file\njson_filename = '/kaggle/input/images/instances_val2017.json'\nwith open(json_filename, 'r') as f:\n    data = json.load(f)\n\ndef download_and_display(image_url, annotations):\n    \"\"\"\n    Downloads an image from a URL, overlays annotations (bounding boxes),\n    and displays the image.\n    \"\"\"\n    try:\n        resp = urllib.request.urlopen(image_url)\n        img_array = np.asarray(bytearray(resp.read()), dtype=np.uint8)\n        img = cv2.imdecode(img_array, cv2.IMREAD_COLOR)\n    except Exception as e:\n        print(f\"Error downloading image: {e}\")\n        return\n\n    # Draw bounding boxes\n    for ann in annotations:\n        x, y, w, h = ann['bbox']\n        cv2.rectangle(img, (int(x), int(y)), (int(x+w), int(y+h)), (255, 0, 0), 2)\n\n    # Convert BGR to RGB for display\n    plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n    plt.axis('off')\n    plt.show()\n\n# Get the number of images to display\nnum_images = int(input(\"Enter the number of images to load (default is all): \") or len(data['images']))\n\nfor i, img_data in enumerate(data['images']):\n    if i >= num_images:\n        break\n\n    # Retrieve image details\n    img_id = img_data['id']\n    img_url = img_data.get('coco_url', None)\n    if not img_url:\n        print(f\"Image {img_id} has no URL.\")\n        continue\n\n    # Get annotations for the image\n    annotations = [\n        ann for ann in data.get('annotations', [])\n        if ann['image_id'] == img_id\n    ]\n\n    # Display the image with annotations\n    download_and_display(img_url, annotations)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T12:43:12.719379Z","iopub.execute_input":"2024-11-21T12:43:12.720059Z","iopub.status.idle":"2024-11-21T12:48:38.411431Z","shell.execute_reply.started":"2024-11-21T12:43:12.720019Z","shell.execute_reply":"2024-11-21T12:48:38.410235Z"}},"outputs":[{"output_type":"stream","name":"stdin","text":"Enter the number of images to load (default is all):  10\n"},{"name":"stdout","text":"Error downloading image: <urlopen error [Errno -3] Temporary failure in name resolution>\nError downloading image: <urlopen error [Errno -3] Temporary failure in name resolution>\nError downloading image: <urlopen error [Errno -3] Temporary failure in name resolution>\nError downloading image: <urlopen error [Errno -3] Temporary failure in name resolution>\nError downloading image: <urlopen error [Errno -3] Temporary failure in name resolution>\nError downloading image: <urlopen error [Errno -3] Temporary failure in name resolution>\nError downloading image: <urlopen error [Errno -3] Temporary failure in name resolution>\nError downloading image: <urlopen error [Errno -3] Temporary failure in name resolution>\nError downloading image: <urlopen error [Errno -3] Temporary failure in name resolution>\nError downloading image: <urlopen error [Errno -3] Temporary failure in name resolution>\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"import cv2\nimport numpy as np\nfrom PIL import Image\n\ndef process_images(original_images, original_annotations):\n    processed_images = []\n    processed_annotations = []\n\n    for img_path, annotation in zip(original_images, original_annotations):\n        # Load the image from a local path\n        img = cv2.imread(img_path)\n\n        # Check if the image was loaded correctly\n        if img is None:\n            print(f\"Error loading image at {img_path}\")\n            continue\n\n        # Convert to grayscale\n        gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n\n        # Apply binary thresholding\n        binary_img = cv2.threshold(gray_img, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)[1]\n\n        # Resize image\n        resized_img = cv2.resize(img, (224, 224))\n\n        # Rotate if height > width\n        height, width, _ = img.shape\n        if height > width:\n            rotated_img = cv2.rotate(img, cv2.ROTATE_90_CLOCKWISE)\n        else:\n            rotated_img = img\n\n        # Sharpen the image to reduce blurriness\n        sharpened_img = cv2.GaussianBlur(rotated_img, (0, 0), sigmaX=1, sigmaY=1)\n        sharpened_img = cv2.addWeighted(rotated_img, 1.5, sharpened_img, -0.5, 0)\n\n        # Convert the sharpened image to grayscale (black and white)\n        bw_img = cv2.cvtColor(sharpened_img, cv2.COLOR_BGR2GRAY)\n\n        # Add processed images and annotations to lists\n        processed_images.extend([img, binary_img, resized_img, rotated_img, sharpened_img, bw_img])\n        processed_annotations.extend([annotation] * 6)\n\n    return processed_images, processed_annotations\n\n# Example image paths and annotations\noriginal_images = [\"/kaggle/input/images/val2017-20241107T150103Z-001/val2017/000000000632.jpg\"]\noriginal_annotations = ['Annotation for image 1']\n\n# Process the images\nprocessed_images, processed_annotations = process_images(original_images, original_annotations)\n\n# Define a placeholder `download_and_display` function\ndef download_and_display(img_path, annotation, image):\n    print(f\"Displaying processed image for {img_path} with annotation: {annotation}\")\n    # Example display using OpenCV (uncomment if running in an environment that supports cv2.imshow)\n    # cv2.imshow(\"Processed Image\", image)\n    # cv2.waitKey(0)\n    # cv2.destroyAllWindows()\n\n# Display the processed images\nfor i, (img, annotation) in enumerate(zip(processed_images, processed_annotations)):\n    download_and_display(original_images[0], annotation, image=img)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T13:33:57.792915Z","iopub.execute_input":"2024-11-21T13:33:57.793335Z","iopub.status.idle":"2024-11-21T13:33:57.829625Z","shell.execute_reply.started":"2024-11-21T13:33:57.793298Z","shell.execute_reply":"2024-11-21T13:33:57.828325Z"}},"outputs":[{"name":"stdout","text":"Displaying processed image for /kaggle/input/images/val2017-20241107T150103Z-001/val2017/000000000632.jpg with annotation: Annotation for image 1\nDisplaying processed image for /kaggle/input/images/val2017-20241107T150103Z-001/val2017/000000000632.jpg with annotation: Annotation for image 1\nDisplaying processed image for /kaggle/input/images/val2017-20241107T150103Z-001/val2017/000000000632.jpg with annotation: Annotation for image 1\nDisplaying processed image for /kaggle/input/images/val2017-20241107T150103Z-001/val2017/000000000632.jpg with annotation: Annotation for image 1\nDisplaying processed image for /kaggle/input/images/val2017-20241107T150103Z-001/val2017/000000000632.jpg with annotation: Annotation for image 1\nDisplaying processed image for /kaggle/input/images/val2017-20241107T150103Z-001/val2017/000000000632.jpg with annotation: Annotation for image 1\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"import os\nimport cv2\nimport torch\nimport numpy as np\nfrom torch.utils.data import Dataset\n\nclass CocoDataset(Dataset):\n    def __init__(self, images, annotations, category_map, image_dir, transform=None):\n        \"\"\"\n        Custom dataset for COCO-like dataset.\n\n        Args:\n            images (list): List of images in the dataset.\n            annotations (list): List of annotations for each image.\n            category_map (dict): Mapping of category IDs to category names.\n            image_dir (str): Directory path where images are stored.\n            transform (callable, optional): Transformations to be applied to images and annotations.\n        \"\"\"\n        self.images = images\n        self.annotations = annotations\n        self.category_map = category_map\n        self.image_dir = image_dir\n        self.transform = transform\n        self.image_annotations = self._map_annotations_to_images()\n\n    def _map_annotations_to_images(self):\n        \"\"\"\n        Groups annotations by image ID.\n\n        Returns:\n            dict: Mapping from image IDs to their respective annotations.\n        \"\"\"\n        annotation_map = {}\n        for annotation in self.annotations:\n            image_id = annotation['image_id']\n            if image_id not in annotation_map:\n                annotation_map[image_id] = []\n            annotation_map[image_id].append(annotation)\n        return annotation_map\n\n    def __len__(self):\n        return len(self.images)\n\n    def __getitem__(self, idx):\n        \"\"\"\n        Returns an image and its annotations as a dictionary.\n\n        Args:\n            idx (int): Index of the image.\n\n        Returns:\n            tuple: (image, target) where image is the transformed image, and target is a dictionary containing bounding boxes and labels.\n        \"\"\"\n        image_info = self.images[idx]\n        img_path = os.path.join(self.image_dir, image_info['file_name'])\n        image = cv2.imread(img_path)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n        # Get annotations for the image\n        image_id = image_info['id']\n        annotations = self.image_annotations.get(image_id, [])\n\n        boxes = []\n        labels = []\n        for ann in annotations:\n            x, y, width, height = ann['bbox']\n            boxes.append([x, y, x + width, y + height])\n            labels.append(ann['category_id'])\n\n        # Convert boxes and labels to numpy arrays\n        boxes = np.array(boxes)\n        labels = np.array(labels)\n\n        # Apply transformations if any\n        if self.transform:\n            transformed = self.transform(image=image, bboxes=boxes, labels=labels)\n            image = transformed['image']\n            boxes = transformed['bboxes']\n            labels = transformed['labels']\n\n        # Convert to PyTorch tensors\n        boxes = torch.tensor(boxes, dtype=torch.float32)\n        labels = torch.tensor(labels, dtype=torch.int64)\n        target = {\"boxes\": boxes, \"labels\": labels}\n\n        return image, target\n\n    def get_image_info(self, idx):\n        \"\"\"\n        Returns metadata for a specific image.\n\n        Args:\n            idx (int): Index of the image.\n\n        Returns:\n            dict: Metadata for the image.\n        \"\"\"\n        return self.images[idx]\n\n    def visualize_image(self, idx):\n        \"\"\"\n        Displays an image along with its bounding boxes.\n\n        Args:\n            idx (int): Index of the image.\n        \"\"\"\n        import matplotlib.pyplot as plt\n        import matplotlib.patches as patches\n\n        image, target = self[idx]\n        boxes = target['boxes'].numpy()\n        labels = target['labels'].numpy()\n\n        fig, ax = plt.subplots(1)\n        ax.imshow(image)\n\n        for box, label in zip(boxes, labels):\n            x, y, x2, y2 = box\n            width, height = x2 - x, y2 - y\n            rect = patches.Rectangle((x, y), width, height, linewidth=2, edgecolor='r', facecolor='none')\n            ax.add_patch(rect)\n            ax.text(x, y, self.category_map[label], color='white', verticalalignment='top', bbox=dict(facecolor='red', alpha=0.5))\n\n        plt.axis('off')\n        plt.show()\n\n    def __repr__(self):\n        return f\"CocoDataset(num_images={len(self.images)}, num_annotations={len(self.annotations)}, image_dir='{self.image_dir}')\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torchvision\nfrom torchvision.models.detection import fasterrcnn_resnet50_fpn\nimport torchvision.transforms as transforms\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nimport os\n\n# COCO dataset class labels\nCOCO_INSTANCE_CATEGORY_NAMES = [\n    '__background__', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat',\n    'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog',\n    'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella',\n    'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite',\n    'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'bottle',\n    'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich',\n    'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch',\n    'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote',\n    'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book',\n    'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush'\n]\n\ndef load_object_detection_model(weights_path=None):\n    \"\"\"\n    Load Faster R-CNN model with local weights\n    \"\"\"\n    try:\n        # Initialize model architecture\n        model = fasterrcnn_resnet50_fpn(pretrained=False, progress=False)\n        \n        # Load weights if path provided\n        if weights_path and os.path.exists(weights_path):\n            model.load_state_dict(torch.load(weights_path))\n        else:\n            print(\"Warning: No weights loaded. Model performance will be random.\")\n        \n        model.eval()  # Set to evaluation mode\n        return model\n    except Exception as e:\n        print(f\"Error loading model: {e}\")\n        return None\n\ndef preprocess_image(image_path):\n    \"\"\"\n    Preprocess image for object detection\n    \"\"\"\n    try:\n        # Image transformation pipeline\n        transform = transforms.Compose([\n            transforms.ToTensor(),\n        ])\n        \n        # Load and transform image\n        img = Image.open(image_path).convert('RGB')\n        img_tensor = transform(img).unsqueeze(0)  # Add batch dimension\n        return img, img_tensor\n    except Exception as e:\n        print(f\"Error preprocessing image: {e}\")\n        return None, None\n\ndef visualize_predictions(img, predictions, threshold=0.5):\n    \"\"\"\n    Visualize object detection predictions\n    \"\"\"\n    try:\n        # Create figure and axes\n        fig, ax = plt.subplots(1, figsize=(12, 9))\n        ax.imshow(img)\n        \n        # Process predictions\n        for box, label, score in zip(predictions['boxes'], predictions['labels'], predictions['scores']):\n            if score > threshold:\n                x1, y1, x2, y2 = box.int().tolist()\n                label_name = COCO_INSTANCE_CATEGORY_NAMES[label]\n                \n                # Draw bounding box\n                rect = patches.Rectangle((x1, y1), x2 - x1, y2 - y1, \n                                         linewidth=2, edgecolor='red', facecolor='none')\n                ax.add_patch(rect)\n                \n                # Add label and confidence\n                ax.text(x1, y1-10, f'{label_name} {score:.2f}', \n                        color='white', fontsize=10,\n                        bbox=dict(facecolor='red', alpha=0.5))\n        \n        plt.axis('off')\n        plt.tight_layout()\n        plt.show()\n    except Exception as e:\n        print(f\"Visualization error: {e}\")\n\ndef main(image_path, weights_path=None):\n    \"\"\"\n    Main object detection pipeline\n    \"\"\"\n    # Load model\n    model = load_object_detection_model(weights_path)\n    if not model:\n        return\n    \n    # Preprocess image\n    orig_img, img_tensor = preprocess_image(image_path)\n    if orig_img is None or img_tensor is None:\n        return\n    \n    # Perform detection\n    with torch.no_grad():\n        predictions = model(img_tensor)[0]\n    \n    # Visualize results\n    visualize_predictions(orig_img, predictions)\n\n# Example usage\nif __name__ == \"__main__\":\n    # Update these paths\n    image_path = \"/kaggle/input/images/val2017-20241107T150103Z-001/val2017/000000000776.jpg\"\n    weights_path = \"/kaggle/input/fasterrcnn/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\"\n    main(image_path, weights_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T13:58:31.590267Z","iopub.execute_input":"2024-11-21T13:58:31.591032Z","iopub.status.idle":"2024-11-21T13:59:04.015683Z","shell.execute_reply.started":"2024-11-21T13:58:31.590990Z","shell.execute_reply":"2024-11-21T13:59:04.014333Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n","output_type":"stream"},{"name":"stdout","text":"Error loading model: <urlopen error [Errno -3] Temporary failure in name resolution>\n","output_type":"stream"}],"execution_count":18}]}