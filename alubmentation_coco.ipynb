{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5qx8OsPcEQhC",
    "outputId": "0ce36026-3559-4c2b-9ab6-bc2a9dc6b975"
   },
   "outputs": [],
   "source": [
    "!pip install -U albumentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "f1tcm619Ntxx"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "SE_nySCqNnLE"
   },
   "outputs": [],
   "source": [
    "json_path = \"C:/Users/megha/Downloads/instances_val2017.json-20241115T150718Z-001\"\n",
    "images_dir = \"C:/Users/megha/Downloads/val2017-20241115T150951Z-001\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CocoDataset(torch.utils.data.Dataset):  # Inheriting from Dataset\n",
    "    def __init__(self, image_metadata, annotation_data, category_mapping, image_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Initializes the dataset with images, annotations, and other required data.\n",
    "\n",
    "        Parameters:\n",
    "            image_metadata (list): A list of image metadata (e.g., file names and image IDs).\n",
    "            annotation_data (list): A list of annotations (bounding boxes and category IDs).\n",
    "            category_mapping (dict): A mapping of category IDs to class names.\n",
    "            image_dir (str): Directory where images are stored.\n",
    "            transform (callable, optional): Transformation to apply to images and annotations.\n",
    "        \"\"\"\n",
    "        self.image_metadata = image_metadata\n",
    "        self.annotation_data = annotation_data\n",
    "        self.category_mapping = category_mapping\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.image_id_to_annotations = self._group_annotations_by_image()\n",
    "\n",
    "    def _group_annotations_by_image(self):\n",
    "        \"\"\"\n",
    "        Groups annotations by image ID for efficient retrieval during data loading.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary mapping image IDs to their annotations.\n",
    "        \"\"\"\n",
    "        image_id_to_annotations = {}\n",
    "        for annotation in self.annotation_data:\n",
    "            image_id = annotation['image_id']\n",
    "            if image_id not in image_id_to_annotations:\n",
    "                image_id_to_annotations[image_id] = []\n",
    "            image_id_to_annotations[image_id].append(annotation)\n",
    "        return image_id_to_annotations\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the number of images in the dataset.\n",
    "\n",
    "        Returns:\n",
    "            int: Total number of images.\n",
    "        \"\"\"\n",
    "        return len(self.image_metadata)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retrieves an image and its corresponding annotations.\n",
    "\n",
    "        Parameters:\n",
    "            idx (int): Index of the image and its annotations.\n",
    "\n",
    "        Returns:\n",
    "            tuple: A tuple containing the image and its target annotations (bounding boxes and labels).\n",
    "        \"\"\"\n",
    "        # Get image information\n",
    "        image_info = self.image_metadata[idx]\n",
    "        img_path = os.path.join(self.image_dir, image_info['file_name'])\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert image to RGB format\n",
    "\n",
    "        # Retrieve annotations for the image\n",
    "        image_id = image_info['id']\n",
    "        annotations = self.image_id_to_annotations.get(image_id, [])\n",
    "\n",
    "        # Initialize lists for bounding boxes and labels\n",
    "        bounding_boxes = []\n",
    "        category_labels = []\n",
    "        for annotation in annotations:\n",
    "            x, y, width, height = annotation['bbox']\n",
    "            bounding_boxes.append([x, y, x + width, y + height])  # Convert to (x1, y1, x2, y2)\n",
    "            category_labels.append(annotation['category_id'])\n",
    "\n",
    "        # Convert bounding boxes and labels to numpy arrays for transformation\n",
    "        bounding_boxes = np.array(bounding_boxes)\n",
    "        category_labels = np.array(category_labels)\n",
    "\n",
    "        # Apply transformations if provided\n",
    "        if self.transform:\n",
    "            transformed = self.transform(image=image, bboxes=bounding_boxes, labels=category_labels)\n",
    "            image = transformed['image']\n",
    "            bounding_boxes = transformed['bboxes']\n",
    "            category_labels = transformed['labels']\n",
    "\n",
    "        # Convert to PyTorch tensors\n",
    "        bounding_boxes = torch.tensor(bounding_boxes, dtype=torch.float32)\n",
    "        category_labels = torch.tensor(category_labels, dtype=torch.int64)\n",
    "        target = {\"boxes\": bounding_boxes, \"labels\": category_labels}\n",
    "\n",
    "        return image, target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "rhQgpoVoQ6Yw"
   },
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mGXrtxPaIJlP",
    "outputId": "bd2519f8-6f5e-47b3-eb93-fa6abfdcbbe3"
   },
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'C:/Users/megha/Downloads/instances_val2017.json-20241115T150718Z-001'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_23012\\2327043385.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;31m# Load annotations (COCO format)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mannotations_json_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m     \u001b[0mcoco_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'C:/Users/megha/Downloads/instances_val2017.json-20241115T150718Z-001'"
     ]
    }
   ],
   "source": [
    "# Augmentation and Preprocessing Pipeline\n",
    "transform_pipeline = A.Compose([\n",
    "    A.Resize(416, 416),  # Resize images to 416x416\n",
    "    A.RandomBrightnessContrast(p=0.2),  # Adjust brightness and contrast\n",
    "    A.GaussianBlur(p=0.2),  # Apply Gaussian blur\n",
    "    A.HorizontalFlip(p=0.5),  # Apply horizontal flip with 50% probability\n",
    "    A.Rotate(limit=20, p=0.5),  # Apply random rotation with a limit of 20 degrees\n",
    "    A.ColorJitter(p=0.3),  # Randomly change image color\n",
    "    A.ToGray(p=0.1),  # Randomly convert some images to grayscale\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),  # Normalize based on ImageNet values\n",
    "    ToTensorV2()  # Convert the image to PyTorch tensor\n",
    "], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['labels']))\n",
    "\n",
    "# Load COCO dataset (images, annotations, category_mapping) from JSON\n",
    "annotations_json_path = r'C:/Users/megha/Downloads/instances_val2017.json-20241115T150718Z-001'\n",
    " #  your file path\n",
    "image_directory =  \"C:/Users/megha/Downloads/val2017-20241115T150951Z-001\"  #  your image directory path\n",
    "\n",
    "# Load annotations (COCO format)\n",
    "with open(annotations_json_path, 'r') as f:\n",
    "    coco_data = json.load(f)\n",
    "\n",
    "# Extract images, annotations, and categories\n",
    "image_list = coco_data['images']  # List of image data\n",
    "annotation_list = coco_data['annotations']  # List of annotations\n",
    "category_list = coco_data['categories']  # List of categories\n",
    "\n",
    "# Create a mapping of category IDs to category names\n",
    "category_id_mapping = {category['id']: category['name'] for category in category_list}\n",
    "\n",
    "# Initialize the custom dataset\n",
    "custom_dataset = CocoDataset(\n",
    "    images=image_list,\n",
    "    annotations=annotation_list,\n",
    "    category_mapping=category_id_mapping,\n",
    "    img_dir=image_directory,\n",
    "    transform=transform_pipeline\n",
    ")\n",
    "\n",
    "# Improved collate_fn to handle dynamic batching and padding for images of varying sizes\n",
    "def dynamic_collate_fn(batch):\n",
    "    img_batch, target_batch = zip(*batch)\n",
    "\n",
    "    # Find the maximum height and width of images in the batch\n",
    "    max_img_height = max([img.shape[1] for img in img_batch])  # Maximum height\n",
    "    max_img_width = max([img.shape[2] for img in img_batch])   # Maximum width\n",
    "\n",
    "    padded_images = []\n",
    "    for img in img_batch:\n",
    "        # Create a tensor of zeros (padding) and copy the image into the top-left corner\n",
    "        padded_img = torch.zeros((3, max_img_height, max_img_width), dtype=torch.float32)\n",
    "        padded_img[:, :img.shape[1], :img.shape[2]] = img\n",
    "        padded_images.append(padded_img)\n",
    "\n",
    "    # Stack the padded images into a single tensor\n",
    "    img_batch = torch.stack(padded_images, dim=0)\n",
    "\n",
    "    return img_batch, target_batch\n",
    "\n",
    "# Set up DataLoader with the dynamic collate function\n",
    "data_loader = DataLoader(custom_dataset, batch_size=8, shuffle=True, collate_fn=dynamic_collate_fn)\n",
    "\n",
    "# Test the data loading process\n",
    "for batch_idx, (images, targets) in enumerate(data_loader):\n",
    "    if batch_idx >= 5:  # Limit the number of batches to test\n",
    "        break\n",
    "    print(f\"Batch {batch_idx} - Image batch size: {images.size()}\")\n",
    "    print(f\"Batch {batch_idx} - Number of targets: {len(targets)}\")\n",
    "    print(f\"Sample target: {targets[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
