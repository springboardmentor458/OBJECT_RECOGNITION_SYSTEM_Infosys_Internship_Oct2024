#
#
# import torch
# import torchvision
# from torchvision import transforms
# from torch.utils.data import DataLoader
# from torchvision.models.detection import fasterrcnn_resnet50_fpn
# from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
# import os
# import numpy as np
# from xml.etree import ElementTree as ET
# from PIL import Image
#
# # Define your dataset class
# class VOCDataset(torch.utils.data.Dataset):
#     def __init__(self, root, transforms=None, limit=None):
#         self.root = root
#         self.transforms = transforms
#         self.image_paths = []
#         self.annotation_paths = []
#
#         # Read images and annotations
#         image_folder = os.path.join(root, 'JPEGImages')
#         annotation_folder = os.path.join(root, 'Annotations')
#
#         for filename in os.listdir(image_folder):
#             if filename.endswith('.jpg'):
#                 image_path = os.path.join(image_folder, filename)
#                 annotation_path = os.path.join(annotation_folder, filename.replace('.jpg', '.xml'))
#                 self.image_paths.append(image_path)
#                 self.annotation_paths.append(annotation_path)
#
#         # Limit the dataset to the specified number of images
#         if limit is not None:
#             self.image_paths = self.image_paths[:limit]
#             self.annotation_paths = self.annotation_paths[:limit]
#
#     def __getitem__(self, idx):
#         image_path = self.image_paths[idx]
#         annotation_path = self.annotation_paths[idx]
#
#         image = Image.open(image_path).convert("RGB")
#         boxes = []
#         labels = []
#
#         # Parse XML annotation
#         tree = ET.parse(annotation_path)
#         root = tree.getroot()
#
#         for obj in root.iter('object'):
#             name = obj.find('name').text
#             label = self.get_label(name)
#
#             bbox = obj.find('bndbox')
#             xmin = float(bbox.find('xmin').text)
#             ymin = float(bbox.find('ymin').text)
#             xmax = float(bbox.find('xmax').text)
#             ymax = float(bbox.find('ymax').text)
#             boxes.append([xmin, ymin, xmax, ymax])
#             labels.append(label)
#
#         boxes = torch.as_tensor(boxes, dtype=torch.float32)
#         labels = torch.as_tensor(labels, dtype=torch.int64)
#
#         # Prepare the target dictionary
#         target = {"boxes": boxes, "labels": labels}
#
#         # Apply transforms if any
#         if self.transforms:
#             image = self.transforms(image)
#
#         return image, target
#
#     def __len__(self):
#         return len(self.image_paths)
#
#     def get_label(self, class_name):
#         # Return class labels for Pascal VOC
#         class_dict = {'aeroplane': 1, 'bicycle': 2, 'bird': 3, 'boat': 4, 'bottle': 5, 'bus': 6, 'car': 7,
#                       'cat': 8, 'chair': 9, 'cow': 10, 'dog': 11, 'horse': 12, 'motorbike': 13, 'person': 14,
#                       'pottedplant': 15, 'sheep': 16, 'sofa': 17, 'train': 18, 'tvmonitor': 19}
#         return class_dict.get(class_name, 0)  # Default to 0 if not found
#
# # Define the transforms
# transform = transforms.Compose([
#     transforms.ToTensor(),
# ])
#
# # Initialize the dataset and dataloader with a limit of 1000 images
# dataset = VOCDataset(
#     root=r'C:\Users\vrunda\Desktop\new\VOC2012_train_val\VOC2012_train_val',
#     transforms=transform,
#     limit=1000
# )
# data_loader = DataLoader(dataset, batch_size=4, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))
#
# # Load the pre-trained Faster R-CNN model
# model = fasterrcnn_resnet50_fpn(weights="DEFAULT")  # Updated weights argument
#
# # Modify the classifier to match the number of classes in Pascal VOC (20 classes + background)
# num_classes = 21  # 20 classes + 1 background
# in_features = model.roi_heads.box_predictor.cls_score.in_features
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)
#
# # Use GPU if available
# device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')
# model.to(device)
#
# # Define optimizer and learning rate scheduler
# optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)
#
# # Training loop
# num_epochs = 5
# for epoch in range(num_epochs):
#     model.train()
#     running_loss = 0.0
#     for images, targets in data_loader:
#         images = [image.to(device) for image in images]
#         targets = [{k: v.to(device) for k, v in t.items()} for t in targets]
#
#         optimizer.zero_grad()
#         loss_dict = model(images, targets)
#
#         losses = sum(loss for loss in loss_dict.values())
#         losses.backward()
#         optimizer.step()
#
#         running_loss += losses.item()
#
#     print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / len(data_loader):.4f}')
#
# # Save the trained model
# torch.save(model.state_dict(), 'fasterrcnn_voc2012_1000_images.pth')
#
import torch
import torchvision
from torchvision.models.detection import fasterrcnn_resnet50_fpn
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
from PIL import Image
import matplotlib.pyplot as plt
import matplotlib.patches as patches
from torchvision import transforms

# Pascal VOC class names (21 classes: 20 classes + 1 background)
class_names = [
    '__background__', 'aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car',
    'cat', 'chair', 'cow', 'dog', 'horse', 'motorbike', 'person', 'pottedplant', 'sheep',
    'sofa', 'train', 'tvmonitor'
]

# Define the transforms
transform = transforms.Compose([
    transforms.ToTensor(),
])

# Load the trained model
model = fasterrcnn_resnet50_fpn(pretrained=False)  # Set pretrained=False since you're loading your custom-trained model
num_classes = 21  # 20 classes + background

# Modify the model's classifier (as done during training)
in_features = model.roi_heads.box_predictor.cls_score.in_features
model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)

# Load the model weights (adjust the path to where you saved your model)
model.load_state_dict(torch.load('fasterrcnn_voc2012_1000_images.pth'))
model.eval()  # Set model to evaluation mode

# Move model to GPU if available
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')
model.to(device)

# Load a sample image (use the path to your image)
image_path = "VOC2012_train_val/VOC2012_train_val/JPEGImages/2007_000241.jpg"  # Update with your image path
image = Image.open(image_path).convert("RGB")

# Apply the same transformations as during training
image_tensor = transform(image).unsqueeze(0).to(device)

# Run inference
with torch.no_grad():
    prediction = model(image_tensor)

# Get prediction details
boxes = prediction[0]['boxes']
labels = prediction[0]['labels']
scores = prediction[0]['scores']

# Filter predictions based on a score threshold (e.g., 0.5)
threshold = 0.5
filtered_boxes = boxes[scores > threshold]
filtered_labels = labels[scores > threshold]

# Visualize the image and bounding boxes
fig, ax = plt.subplots(1, figsize=(12, 9))
ax.imshow(image)

# Draw bounding boxes on the image
for box, label in zip(filtered_boxes, filtered_labels):
    xmin, ymin, xmax, ymax = box.tolist()
    rect = patches.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin, linewidth=2, edgecolor='r', facecolor='none')
    ax.add_patch(rect)

    # Map label to class name and display it
    class_name = class_names[label.item()]
    ax.text(xmin, ymin, class_name, fontsize=12, color='white', bbox=dict(facecolor='red', alpha=0.5))

plt.show()





