{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OdegMgakaqt0"
      },
      "source": [
        "###albumentations\n",
        "###transform pipeline uses albumentations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sYI5K-mFZpKF",
        "outputId": "96bcb680-b881-4b99-f7d4-b63e33cbe567",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: albumentations in /usr/local/lib/python3.10/dist-packages (1.4.20)\n",
            "Requirement already satisfied: numpy>=1.24.4 in /usr/local/lib/python3.10/dist-packages (from albumentations) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from albumentations) (1.13.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from albumentations) (6.0.2)\n",
            "Requirement already satisfied: pydantic>=2.7.0 in /usr/local/lib/python3.10/dist-packages (from albumentations) (2.9.2)\n",
            "Requirement already satisfied: albucore==0.0.19 in /usr/local/lib/python3.10/dist-packages (from albumentations) (0.0.19)\n",
            "Requirement already satisfied: eval-type-backport in /usr/local/lib/python3.10/dist-packages (from albumentations) (0.2.0)\n",
            "Requirement already satisfied: opencv-python-headless>=4.9.0.80 in /usr/local/lib/python3.10/dist-packages (from albumentations) (4.10.0.84)\n",
            "Requirement already satisfied: stringzilla>=3.10.4 in /usr/local/lib/python3.10/dist-packages (from albucore==0.0.19->albumentations) (3.10.10)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.7.0->albumentations) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.7.0->albumentations) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.7.0->albumentations) (4.12.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (11.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: albumentations in /usr/local/lib/python3.10/dist-packages (1.4.20)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\n",
            "Requirement already satisfied: numpy>=1.24.4 in /usr/local/lib/python3.10/dist-packages (from albumentations) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from albumentations) (1.13.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from albumentations) (6.0.2)\n",
            "Requirement already satisfied: pydantic>=2.7.0 in /usr/local/lib/python3.10/dist-packages (from albumentations) (2.9.2)\n",
            "Requirement already satisfied: albucore==0.0.19 in /usr/local/lib/python3.10/dist-packages (from albumentations) (0.0.19)\n",
            "Requirement already satisfied: eval-type-backport in /usr/local/lib/python3.10/dist-packages (from albumentations) (0.2.0)\n",
            "Requirement already satisfied: opencv-python-headless>=4.9.0.80 in /usr/local/lib/python3.10/dist-packages (from albumentations) (4.10.0.84)\n",
            "Requirement already satisfied: stringzilla>=3.10.4 in /usr/local/lib/python3.10/dist-packages (from albucore==0.0.19->albumentations) (3.10.10)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (11.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.7.0->albumentations) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.7.0->albumentations) (2.23.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install albumentations\n",
        "!pip install torch torchvision\n",
        "!pip install albumentations torch torchvision\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ASbQVIxBZ1Vk",
        "outputId": "d9f64b95-1b42-4722-bbf7-6d9c8b50829d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/albumentations/__init__.py:24: UserWarning: A new version of Albumentations is available: 1.4.21 (you have 1.4.20). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
            "  check_for_updates()\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z5mmE77lkzjp"
      },
      "outputs": [],
      "source": [
        "json_path = '/content/drive/MyDrive/Object Recognition Dataset/instances_val2017.json/instances_val2017.json'\n",
        "images_dir = '/content/drive/MyDrive/Object Recognition Dataset/val2017'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bSlfsGtfZ3qV"
      },
      "outputs": [],
      "source": [
        "# Define the CocoDataset class\n",
        "class CocoDataset(Dataset):\n",
        "    def __init__(self, images, annotations, category_mapping, img_dir, transform=None):\n",
        "        self.images = images\n",
        "        self.annotations = annotations\n",
        "        self.category_mapping = category_mapping\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "        self.image_id_to_annotations = self._group_annotations_by_image()  # Fix here\n",
        "\n",
        "    def _group_annotations_by_image(self):\n",
        "        image_id_to_annotations = {}\n",
        "        for ann in self.annotations:\n",
        "            image_id = ann['image_id']\n",
        "            if image_id not in image_id_to_annotations:\n",
        "                image_id_to_annotations[image_id] = []\n",
        "            image_id_to_annotations[image_id].append(ann)\n",
        "        return image_id_to_annotations\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_info = self.images[idx]\n",
        "        img_path = os.path.join(self.img_dir, image_info['file_name'])\n",
        "        image = cv2.imread(img_path)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert to RGB\n",
        "\n",
        "        # Get annotations\n",
        "        image_id = image_info['id']\n",
        "        annotations = self.image_id_to_annotations.get(image_id, [])\n",
        "\n",
        "        boxes = []\n",
        "        labels = []\n",
        "        for ann in annotations:\n",
        "            x, y, width, height = ann['bbox']\n",
        "            boxes.append([x, y, x + width, y + height])\n",
        "            labels.append(ann['category_id'])\n",
        "\n",
        "        # Convert boxes and labels to numpy arrays for Albumentations\n",
        "        boxes = np.array(boxes)\n",
        "        labels = np.array(labels)\n",
        "\n",
        "        # Apply transformations\n",
        "        if self.transform:\n",
        "            transformed = self.transform(image=image, bboxes=boxes, labels=labels)\n",
        "            image = transformed['image']\n",
        "            boxes = transformed['bboxes']\n",
        "            labels = transformed['labels']\n",
        "\n",
        "        # Convert to PyTorch tensors\n",
        "        boxes = torch.tensor(boxes, dtype=torch.float32)\n",
        "        labels = torch.tensor(labels, dtype=torch.int64)\n",
        "        target = {\"boxes\": boxes, \"labels\": labels}\n",
        "\n",
        "        return image, target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XpVJ1su_-FS9"
      },
      "outputs": [],
      "source": [
        "import torchvision\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from torchvision.models.detection import FasterRCNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b2FAE81Kym6k"
      },
      "outputs": [],
      "source": [
        "# Augmentation and Preprocessing Pipeline\n",
        "transform = A.Compose([\n",
        "    A.Resize(416, 416),  # Resizing\n",
        "    A.RandomBrightnessContrast(p=0.2),  # Brightness and Contrast Adjustment\n",
        "    A.GaussianBlur(p=0.2),  # Blurring for image quality improvement\n",
        "    A.HorizontalFlip(p=0.5),  # Horizontal Flip\n",
        "    A.Rotate(limit=20, p=0.5),  # Random Rotation\n",
        "    A.ColorJitter(p=0.3),  # Random color adjustments\n",
        "    A.ToGray(p=0.1),  # Randomly convert some images to grayscale\n",
        "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),  # Normalization\n",
        "    ToTensorV2()  # Convert image to PyTorch tensor\n",
        "], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['labels']))\n",
        "\n",
        "# Load COCO dataset (images, annotations, category_mapping) from JSON\n",
        "annotations_file = '/content/drive/MyDrive/Object Recognition Dataset/instances_val2017.json/instances_val2017.json'\n",
        "img_dir = '/content/drive/MyDrive/Object Recognition Dataset/val2017'\n",
        "\n",
        "# Load annotations (COCO-style)\n",
        "with open(annotations_file, 'r') as f:\n",
        "    coco_data = json.load(f)\n",
        "\n",
        "images = coco_data['images']\n",
        "annotations = coco_data['annotations']\n",
        "categories = coco_data['categories']\n",
        "\n",
        "# Create category mapping (optional)\n",
        "category_mapping = {category['id']: category['name'] for category in categories}\n",
        "\n",
        "# Initialize dataset\n",
        "dataset = CocoDataset(images=images, annotations=annotations, category_mapping=category_mapping, img_dir=img_dir, transform=transform)\n",
        "\n",
        "# Define collate_fn to handle variable-sized inputs\n",
        "def collate_fn(batch):\n",
        "    images, targets = zip(*batch)\n",
        "\n",
        "    # Handle variable image sizes and padding if necessary\n",
        "    max_height = max([img.shape[1] for img in images])  # Find the max height\n",
        "    max_width = max([img.shape[2] for img in images])   # Find the max width\n",
        "\n",
        "    padded_images = []\n",
        "    for img in images:\n",
        "        # Pad images to the maximum width and height\n",
        "        padded_img = torch.zeros((3, max_height, max_width), dtype=torch.float32)\n",
        "        padded_img[:, :img.shape[1], :img.shape[2]] = img\n",
        "        padded_images.append(padded_img)\n",
        "\n",
        "    images = torch.stack(padded_images, dim=0)  # Stack images to create a batch\n",
        "\n",
        "    return images, targets\n",
        "\n",
        "\n",
        "# DataLoader\n",
        "#data_loader = DataLoader(dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)\n",
        "data_loader = DataLoader(dataset, batch_size=6, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GZkB8zT2-G6a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e44711d1-8b05-4e74-af59-7dd86675ea39"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\" to /root/.cache/torch/hub/checkpoints/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\n",
            "100%|██████████| 160M/160M [00:01<00:00, 102MB/s]\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from torchvision.models.detection import FasterRCNN\n",
        "from torchvision.models.detection.faster_rcnn import FasterRCNN_ResNet50_FPN_Weights\n",
        "\n",
        "def create_custom_faster_rcnn(num_classes):\n",
        "    #Download the pretrained  FR-CNN model\n",
        "    weights= FasterRCNN_ResNet50_FPN_Weights.DEFAULT\n",
        "    model= torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=weights)\n",
        "\n",
        "    # Get the number of input features for the classifier\n",
        "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "\n",
        "\n",
        "    # Replace the classifier with a new one (based on the number of classes)\n",
        "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "\n",
        "    return model\n",
        "\n",
        "device= torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "num_classes = 91\n",
        "model = create_custom_faster_rcnn(num_classes)\n",
        "\n",
        "# Move the model to the available device\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kyibehebBepn"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.models.detection import FasterRCNN\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "import time\n",
        "\n",
        "# Define the optimizer (using Adam optimizer for Faster R-CNN)\n",
        "params = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = optim.Adam(params, lr=1e-4)\n",
        "\n",
        "# Define the learning rate scheduler (optional)\n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
        "\n",
        "# Define the loss function for Faster R-CNN (included in the model itself)\n",
        "\n",
        "# Training Loop\n",
        "num_epochs = 10  # Number of epochs to train\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()  # Set the model to training mode\n",
        "    start_time = time.time()\n",
        "    running_loss = 0.0\n",
        "    for images, targets in data_loader:\n",
        "        # Move images and targets to the device (GPU/CPU)\n",
        "        images = [image.to(device) for image in images]\n",
        "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "        # Zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        loss_dict = model(images, targets)\n",
        "\n",
        "        # Get total loss\n",
        "        losses = sum(loss for loss in loss_dict.values())\n",
        "\n",
        "        # Backward pass and optimize\n",
        "        losses.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += losses.item()\n",
        "\n",
        "    # Step the learning rate scheduler\n",
        "    lr_scheduler.step()\n",
        "\n",
        "    epoch_loss = running_loss / len(data_loader)\n",
        "    epoch_time = time.time() - start_time\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}, Time: {epoch_time:.2f}s\")\n",
        "\n",
        "    # Save the model\n",
        "    torch.save(model.state_dict(), f\"faster_rcnn_epoch_{epoch+1}.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation Mode\n",
        "model.eval()\n",
        "test_image, test_target = dataset[0]  # Get one sample\n",
        "test_image = test_image.to(device).unsqueeze(0)\n",
        "\n",
        "# Prediction\n",
        "with torch.no_grad():\n",
        "    predictions = model(test_image)\n",
        "\n",
        "# Visualization\n",
        "pred_image = test_image[0].permute(1, 2, 0).cpu().numpy()\n",
        "pred_image = (pred_image * 255).astype(np.uint8)\n",
        "\n",
        "for box, label, mask in zip(predictions[0]['boxes'], predictions[0]['labels'], predictions[0]['masks']):\n",
        "    x1, y1, x2, y2 = map(int, box)\n",
        "    color = (0, 255, 0)\n",
        "    cv2.rectangle(pred_image, (x1, y1), (x2, y2), color, 2)\n",
        "    text = category_mapping.get(label.item(), \"Unknown\")\n",
        "    cv2.putText(pred_image, text, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1)\n",
        "    mask = mask[0].cpu().numpy() > 0.5\n",
        "    pred_image[mask] = pred_image[mask] * 0.5 + np.array([0, 255, 0], dtype=np.uint8) * 0.5\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.imshow(pred_image)\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "43hwHNCAZF49"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}