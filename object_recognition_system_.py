# -*- coding: utf-8 -*-
"""Object Recognition System .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bmcSRDgwhgmIPFQmqixkRvic0a2FNoVO
"""

import cv2
import os
import json
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from google.colab import drive
drive.mount('/content/drive/')

drive_path ='/content/drive/MyDrive/Colab Notebooks/Object Recognition Dataset'

json_file_path = '/content/drive/MyDrive/Colab Notebooks/Object Recognition Dataset/instances_val2017.json/instances_val2017.json'

with open(json_file_path, "r") as json_file:
    metadata = json.load(json_file)

print("JSON metadata loaded successfully.")

# Load line-delimited JSON file (if applicable)
metadata = []
with open(json_file_path, 'r') as json_file:
    for line in json_file:
        metadata.append(json.loads(line.strip()))

print("Line-delimited metadata loaded successfully.")

image_folder_path = '/content/drive/MyDrive/Colab Notebooks/Object Recognition Dataset/val2017'

valid_extensions = (".jpg", ".jpeg", ".png", ".bmp", ".tiff")

# List to hold loaded images
images = []

# Load each image in the folder
for filename in os.listdir(image_folder_path):
    if filename.lower().endswith(valid_extensions):
        # Construct the full path to the image file
        image_path = os.path.join(image_folder_path, filename)

        # Read the image using OpenCV
        image = cv2.imread(image_path)

        # Check if image is loaded successfully
        if image is not None:
            images.append((filename, image))  # Store tuple of filename and image data
        else:
            print(f"Error loading image: {filename}")

print(f"Loaded {len(images)} images successfully.")

image_path = '/content/drive/MyDrive/Colab Notebooks/000000000776.jpg'
# Load the image
image = cv2.imread(image_path)

# Check if image loaded successfully
if image is not None:
    # Display the image
    plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))
    plt.axis('off')  # Hide axes
    plt.show()
else:
    print("Error: Image not found or could not be loaded.")

# Set the desired standard size
standard_width = 224
standard_height = 224
resized_image = cv2.resize(image, (standard_width, standard_height))

# Display the resized image
plt.imshow(cv2.cvtColor(resized_image, cv2.COLOR_BGR2RGB))
plt.axis('off')
plt.show()

hsv_image = cv2.cvtColor(resized_image, cv2.COLOR_BGR2HSV)

# Define color range for object detection (example: red color)
lower_red = np.array([0, 120, 70])
upper_red = np.array([10, 255, 255])

# Create a mask for the red color range
mask = cv2.inRange(hsv_image, lower_red, upper_red)

# Find contours on the mask
contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

# Draw bounding boxes around each detected contour
for contour in contours:
    x, y, w, h = cv2.boundingRect(contour)  # Get bounding box coordinates
    cv2.rectangle(resized_image, (x, y), (x + w, y + h), (0, 255, 0), 2)  # Draw bounding box

# Display the image with bounding boxes
plt.imshow(cv2.cvtColor(resized_image, cv2.COLOR_BGR2RGB))
plt.axis('off')
plt.show()

mask = cv2.inRange(hsv_image, lower_red, upper_red)

# Optional: apply blurring and morphological transformations to reduce noise
mask = cv2.GaussianBlur(mask, (5, 5), 0)  # Apply Gaussian blur
mask = cv2.dilate(mask, None, iterations=2)  # Dilation to enhance object area
mask = cv2.erode(mask, None, iterations=1)   # Erosion to reduce noise

# Display the mask for debugging
plt.imshow(mask, cmap='gray')
plt.title("Mask for Detected Color")
plt.axis('off')
plt.show()

# Find contours on the cleaned mask
contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

# Draw bounding boxes on the resized image
for contour in contours:
    x, y, w, h = cv2.boundingRect(contour)
    cv2.rectangle(resized_image, (x, y), (x + w, y + h), (0, 255, 0), 2)  # Green bounding box

# Display the result
plt.imshow(cv2.cvtColor(resized_image, cv2.COLOR_BGR2RGB))
plt.axis('off')
plt.title("Image with Bounding Boxes")
plt.show()

image_path = '/content/drive/MyDrive/Colab Notebooks/000000000632.jpg'
# Load the image
image2 = cv2.imread(image_path)

# 1. Original Image with Bounding Boxes
bounding_boxes = [(50, 100, 150, 200), (250, 200, 100, 150)]  # Example bounding boxes
image_with_boxes = image2.copy()
for (x, y, w, h) in bounding_boxes:
    cv2.rectangle(image_with_boxes, (x, y), (x + w, y + h), (0, 255, 0), 2)

# 2. Color-Coded Bounding Boxes
confidence_scores = [0.9, 0.5]  # High and low confidence example
image_with_conf_boxes = image2.copy()
for i, (x, y, w, h) in enumerate(bounding_boxes):
    color = (0, 255, 0) if confidence_scores[i] > 0.7 else (0, 0, 255)
    cv2.rectangle(image_with_conf_boxes, (x, y), (x + w, y + h), color, 2)
    label = f"Confidence: {confidence_scores[i]:.2f}"
    cv2.putText(image_with_conf_boxes, label, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)

# 3. Heatmap Overlay for Attention
heatmap = np.zeros_like(image2[:, :, 0], dtype=np.uint8)
cv2.rectangle(heatmap, (50, 100), (200, 300), 255, -1)  # Sample heat region
colored_heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)
overlayed_image = cv2.addWeighted(image2, 0.6, colored_heatmap, 0.4, 0)

# 4. Segmentation Mask Overlay
mask = np.zeros_like(image2[:, :, 0], dtype=np.uint8)
cv2.rectangle(mask, (50, 100), (200, 300), 255, -1)
masked_image = cv2.addWeighted(image2, 0.6, cv2.merge([mask, mask, mask]), 0.4, 0)

# --- Display in a 2x2 Grid ---

# Set up a 2x2 grid for visualizations
fig, axs = plt.subplots(2, 2, figsize=(10, 10))

# Display each visualization in a separate subplot
axs[0, 0].imshow(cv2.cvtColor(image_with_boxes, cv2.COLOR_BGR2RGB))
axs[0, 0].set_title("Bounding Boxes")
axs[0, 0].axis('off')

axs[0, 1].imshow(cv2.cvtColor(image_with_conf_boxes, cv2.COLOR_BGR2RGB))
axs[0, 1].set_title("Color-coded Bounding Boxes")
axs[0, 1].axis('off')

axs[1, 0].imshow(cv2.cvtColor(overlayed_image, cv2.COLOR_BGR2RGB))
axs[1, 0].set_title("Heatmap Overlay")
axs[1, 0].axis('off')

axs[1, 1].imshow(cv2.cvtColor(masked_image, cv2.COLOR_BGR2RGB))
axs[1, 1].set_title("Segmentation Mask Overlay")
axs[1, 1].axis('off')

# Display the grid of images
plt.tight_layout()
plt.show()

import cv2
from google.colab.patches import cv2_imshow

# Load the image
image = cv2.imread('/content/drive/MyDrive/Colab Notebooks/000000000632.jpg')  # Replace with your image path

# Convert to grayscale
grayscale_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)

# Display the grayscale image
cv2_imshow(grayscale_image)

# Load the image
image = cv2.imread('/content/drive/MyDrive/Colab Notebooks/000000000632.jpg')  # Replace with your image path

# Resize the image to a standard size (224x224)
standard_size = (224, 224)
resized_image = cv2.resize(image, standard_size)
cv2_imshow(resized_image)  # Display the resized image

# Rotate the resized image 90 degrees clockwise
rotated_90 = cv2.rotate(resized_image, cv2.ROTATE_90_CLOCKWISE)
cv2_imshow(rotated_90)

# Rotate the resized image 180 degrees
rotated_180 = cv2.rotate(resized_image, cv2.ROTATE_180)
cv2_imshow(rotated_180)

# Rotate the resized image 270 degrees clockwise (90 degrees counter-clockwise)
rotated_270 = cv2.rotate(resized_image, cv2.ROTATE_90_COUNTERCLOCKWISE)
cv2_imshow(rotated_270)

import os
import cv2
import numpy as np
import torch
from torch.utils.data import Dataset, DataLoader
import albumentations as A
from albumentations.pytorch import ToTensorV2
import json

# Define the CocoDataset class
class CocoDataset(Dataset):
    def __init__(self, images, annotations, category_mapping, img_dir, transform=None):
        self.images = images
        self.annotations = annotations
        self.category_mapping = category_mapping
        self.img_dir = img_dir
        self.transform = transform
        self.image_id_to_annotations = self._group_annotations_by_image()

    def _group_annotations_by_image(self):
        image_id_to_annotations = {}
        for ann in self.annotations:
            image_id = ann['image_id']
            if image_id not in image_id_to_annotations:
                image_id_to_annotations[image_id] = []
            image_id_to_annotations[image_id].append(ann)
        return image_id_to_annotations

    def __len__(self):
        return len(self.images)

    def __getitem__(self, idx):
        image_info = self.images[idx]
        img_path = os.path.join(self.img_dir, image_info['file_name'])
        image = cv2.imread(img_path)
        if image is None:
            raise FileNotFoundError(f"Image {img_path} not found.")
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert to RGB

        # Get annotations
        image_id = image_info['id']
        annotations = self.image_id_to_annotations.get(image_id, [])

        boxes = []
        labels = []
        for ann in annotations:
            x, y, width, height = ann['bbox']
            boxes.append([x, y, x + width, y + height])
            labels.append(ann['category_id'])

        # Convert boxes and labels to numpy arrays for Albumentations
        boxes = np.array(boxes, dtype=np.float32)
        labels = np.array(labels, dtype=np.int64)

        # Apply transformations
        if self.transform:
            transformed = self.transform(image=image, bboxes=boxes, labels=labels)
            image = transformed['image']
            boxes = transformed['bboxes']
            labels = transformed['labels']

        # Convert to PyTorch tensors
        boxes = torch.tensor(boxes, dtype=torch.float32)
        labels = torch.tensor(labels, dtype=torch.int64)
        target = {"boxes": boxes, "labels": labels}

        return image, target

# Custom collate function for DataLoader
def collate_fn(batch):
    images, targets = zip(*batch)
    return list(images), list(targets)

# Augmentation and Preprocessing Pipeline
transform = A.Compose([
    A.Resize(416, 416),  # Resizing
    A.RandomBrightnessContrast(p=0.2),  # Brightness and Contrast Adjustment
    A.GaussianBlur(p=0.2),  # Blurring for image quality improvement
    A.HorizontalFlip(p=0.5),  # Horizontal Flip
    A.Rotate(limit=20, p=0.5),  # Random Rotation
    A.ColorJitter(p=0.3),  # Random color adjustments
    A.ToGray(p=0.1),  # Randomly convert some images to grayscale
    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),  # Normalization
    ToTensorV2()  # Convert image to PyTorch tensor
], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['labels']))

# Path to your directories
img_dir ='/content/drive/MyDrive/Colab Notebooks/Object Recognition Dataset/val2017'
annotations_file = '/content/drive/MyDrive/Colab Notebooks/Object Recognition Dataset/instances_val2017.json/instances_val2017.json'

# Load COCO-style annotations
with open(annotations_file, 'r') as f:
    coco_data = json.load(f)

images = coco_data['images']  # List of image metadata
annotations = coco_data['annotations']  # List of annotations
categories = coco_data['categories']  # List of category metadata

# Create a category mapping
category_mapping = {cat['id']: cat['name'] for cat in categories}

# Create the dataset and DataLoader
dataset = CocoDataset(images, annotations, category_mapping, img_dir, transform=transform)
data_loader = DataLoader(dataset, batch_size=16, shuffle=True, collate_fn=collate_fn)

# Test data loading
for images, targets in data_loader:
    print(f"Images batch size: {len(images)}")
    print(f"Target batch size: {len(targets)}")
    print("Sample target:", targets[0])  # Print sample target (bounding boxes and labels)
    break

#Image Padding

def pad_image(image, target_size, padding_color=(0, 0, 0)):
    """
    Pads an image to the desired size with a specified padding color.
    Args:
        image (numpy array): Input image.
        target_size (tuple): Desired size as (height, width).
        padding_color (tuple): RGB color for padding (default: black).
    Returns:
        numpy array: Padded image.
    """
    h, w = image.shape[:2]
    delta_h = max(0, target_size[0] - h)
    delta_w = max(0, target_size[1] - w)
    top, bottom = delta_h // 2, delta_h - delta_h // 2
    left, right = delta_w // 2, delta_w - delta_w // 2
    return cv2.copyMakeBorder(image, top, bottom, left, right, cv2.BORDER_CONSTANT, value=padding_color)

#Testing
# Load a sample image
image_path = '/content/drive/MyDrive/Colab Notebooks/000000000632.jpg'
image = cv2.imread(image_path)

# Target size for padding
target_size = (640, 640)

# Test padding function
padded_image = pad_image(image, target_size)

# Display the original and padded images
print("Original Image Shape:", image.shape)
print("Padded Image Shape:", padded_image.shape)

# Optionally display using matplotlib
import matplotlib.pyplot as plt
plt.subplot(1, 2, 1)
plt.title("Original Image")
plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))

plt.subplot(1, 2, 2)
plt.title("Padded Image")
plt.imshow(cv2.cvtColor(padded_image, cv2.COLOR_BGR2RGB))
plt.show()

from sklearn.preprocessing import LabelEncoder, OneHotEncoder
import numpy as np  # Make sure to import numpy

def encode_labels(labels, method='onehot'):
    """
    Encodes categorical labels using the specified method.
    Args:
        labels (list): List of categorical labels.
        method (str): Encoding method ('label' or 'onehot').
    Returns:
        numpy array: Encoded labels.
    """
    if method == 'label':
        encoder = LabelEncoder()
        return encoder.fit_transform(labels)
    elif method == 'onehot':
        encoder = OneHotEncoder(sparse_output=False)  # Updated to use sparse_output
        return encoder.fit_transform(np.array(labels).reshape(-1, 1))
    else:
        raise ValueError("Method should be 'label' or 'onehot'")

# Sample labels
labels = ['cat', 'dog', 'bird', 'person', 'table']

# Test label encoding
label_encoded = encode_labels(labels, method='label')
print("Label Encoded:", label_encoded)

# Test one-hot encoding
onehot_encoded = encode_labels(labels, method='onehot')
print("One-Hot Encoded:\n", onehot_encoded)

def dynamic_patching(image, patch_size):
    """
    Splits the image into patches of the given size.
    Args:
        image (numpy array): Input image.
        patch_size (tuple): Patch size as (height, width).
    Returns:
        list: List of image patches as numpy arrays.
    """
    h, w = image.shape[:2]
    patches = []
    for i in range(0, h, patch_size[0]):
        for j in range(0, w, patch_size[1]):
            patch = image[i:i + patch_size[0], j:j + patch_size[1]]
            patches.append(patch)
    return patches

# Define patch size
patch_size = (64, 64)

# Test dynamic patching
patches = dynamic_patching(image, patch_size)

# Print the number of patches and shape of the first patch
print("Number of Patches:", len(patches))
if patches:
    print("Shape of First Patch:", patches[0].shape)

# Optionally visualize a few patches
for i in range(min(len(patches), 5)):  # Display first 5 patches
    plt.subplot(1, 5, i + 1)
    plt.title(f"Patch {i + 1}")
    plt.imshow(cv2.cvtColor(patches[i], cv2.COLOR_BGR2RGB))
plt.show()

from google.colab import drive
drive.mount('/content/drive')

def dynamic_patching(image, patch_size):
    """
    Splits the image into patches of the given size.
    Args:
        image (numpy array): Input image.
        patch_size (tuple): Patch size as (height, width).
    Returns:
        list: List of image patches as numpy arrays.
    """
    h, w = image.shape[:2]
    patches = []
    for i in range(0, h, patch_size[0]):
        for j in range(0, w, patch_size[1]):
            patch = image[i:i + patch_size[0], j:j + patch_size[1]]
            patches.append(patch)
    return patches

# Define patch size
patch_size = (64, 64)

# Test dynamic patching (Make sure 'image' is a valid numpy array)
patches = dynamic_patching(image, patch_size)

# Print the number of patches and shape of the first patch
print("Number of Patches:", len(patches))
if patches:
    print("Shape of First Patch:", patches[0].shape)

# Visualize all patches (loop through all of them)
for i, patch in enumerate(patches):  # Iterate through all patches
    plt.subplot(10, 10, i + 1)  # Adjust number of rows and columns as needed
    plt.title(f"Patch {i}")
    plt.imshow(cv2.cvtColor(patch, cv2.COLOR_BGR2RGB))  # Convert BGR to RGB for display
    plt.axis('off')  # Hide axes for better visualization

plt.show()

import cv2
import numpy as np
import torch
import torchvision
from PIL import Image
from google.colab.patches import cv2_imshow

# Load the pretrained Faster R-CNN model (with COCO weights)
model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)
model.eval()  # Set the model to evaluation mode

# COCO category names
coco_names = [
    "__background__", "person", "bicycle", "car", "motorcycle", "airplane", "bus", "train",
    "truck", "boat", "traffic light", "fire hydrant", "N/A", "stop sign", "parking meter",
    "bench", "bird", "cat", "dog", "horse", "sheep", "cow", "elephant", "bear", "zebra",
    "giraffe", "N/A", "backpack", "umbrella", "N/A", "N/A", "handbag", "tie", "suitcase",
    "frisbee", "skis", "snowboard", "sports ball", "kite", "baseball bat", "baseball glove",
    "skateboard", "surfboard", "tennis racket", "bottle", "N/A", "wine glass", "cup", "fork",
    "knife", "spoon", "bowl", "banana", "apple", "sandwich", "orange", "broccoli", "carrot",
    "hot dog", "pizza", "donut", "cake", "chair", "couch", "potted plant", "bed", "N/A",
    "dining table", "N/A", "N/A", "toilet", "N/A", "tv", "laptop", "mouse", "remote", "keyboard",
    "cell phone", "microwave", "oven", "toaster", "sink", "refrigerator", "N/A", "book", "clock",
    "vase", "scissors", "teddy bear", "hair drier", "toothbrush"
]

# Load the image from file
image_path =  '/content/drive/MyDrive/Colab Notebooks/000000000776.jpg'  # Replace with your image path
image = Image.open(image_path)

# Transformation pipeline to prepare the image for the model
transform = torchvision.transforms.Compose([
    torchvision.transforms.ToTensor()  # Convert image to tensor
])

img = transform(image).unsqueeze(0)  # Add batch dimension

# Perform inference
with torch.no_grad():
    predictions = model(img)

# Extract bounding boxes, labels, and scores from the predictions
bboxes = predictions[0]['boxes']
labels = predictions[0]['labels']
scores = predictions[0]['scores']

# Convert the image to a format that OpenCV can display
image_cv = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)

# Define font for displaying labels
font = cv2.FONT_HERSHEY_SIMPLEX

# Draw bounding boxes on the image
for i in range(len(bboxes)):
    # Get the bounding box coordinates and convert them to integers
    x1, y1, x2, y2 = bboxes[i].numpy().astype("int")

    # Get the label index and check if it's valid
    label_index = labels[i].item()
    if 0 <= label_index < len(coco_names):
        label = coco_names[label_index]  # Get the name from coco_names
        confidence = scores[i].item()

        # Only draw bounding box if confidence is above a threshold
        if confidence > 0.5:
            print(f"Label: {label}, Confidence: {confidence:.2f}")

            # Draw the bounding box
            cv2.rectangle(image_cv, (x1, y1), (x2, y2), (255, 0, 0), 2)
            # Put the label text
            cv2.putText(image_cv, f"{label}: {confidence:.2f}",
                        (x1, y1 - 10), font, 0.5, (255, 0, 0), 1)

# Display the image with bounding boxes
cv2_imshow(image_cv)  # This is used for displaying images in Google Colab

import cv2
import numpy as np
import torch
import torchvision
from PIL import Image
from google.colab.patches import cv2_imshow

# Load the pretrained Faster R-CNN model (with COCO weights)
model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)
model.eval()  # Set the model to evaluation mode

# COCO category names
coco_names = [
    "__background__", "person", "bicycle", "car", "motorcycle", "airplane", "bus", "train",
    "truck", "boat", "traffic light", "fire hydrant", "N/A", "stop sign", "parking meter",
    "bench", "bird", "cat", "dog", "horse", "sheep", "cow", "elephant", "bear", "zebra",
    "giraffe", "N/A", "backpack", "umbrella", "N/A", "N/A", "handbag", "tie", "suitcase",
    "frisbee", "skis", "snowboard", "sports ball", "kite", "baseball bat", "baseball glove",
    "skateboard", "surfboard", "tennis racket", "bottle", "N/A", "wine glass", "cup", "fork",
    "knife", "spoon", "bowl", "banana", "apple", "sandwich", "orange", "broccoli", "carrot",
    "hot dog", "pizza", "donut", "cake", "chair", "couch", "potted plant", "bed", "N/A",
    "dining table", "N/A", "N/A", "toilet", "N/A", "tv", "laptop", "mouse", "remote", "keyboard",
    "cell phone", "microwave", "oven", "toaster", "sink", "refrigerator", "N/A", "book", "clock",
    "vase", "scissors", "teddy bear", "hair drier", "toothbrush"
]

# Load the image from file
image_path =  '/content/drive/MyDrive/Colab Notebooks/000000000632.jpg'  # Replace with your image path
image = Image.open(image_path)

# Transformation pipeline to prepare the image for the model
transform = torchvision.transforms.Compose([
    torchvision.transforms.ToTensor()  # Convert image to tensor
])

img = transform(image).unsqueeze(0)  # Add batch dimension

# Perform inference
with torch.no_grad():
    predictions = model(img)

# Extract bounding boxes, labels, and scores from the predictions
bboxes = predictions[0]['boxes']
labels = predictions[0]['labels']
scores = predictions[0]['scores']

# Convert the image to a format that OpenCV can display
image_cv = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)

# Define font for displaying labels
font = cv2.FONT_HERSHEY_SIMPLEX

# Draw bounding boxes on the image
for i in range(len(bboxes)):
    # Get the bounding box coordinates and convert them to integers
    x1, y1, x2, y2 = bboxes[i].numpy().astype("int")

    # Get the label index and check if it's valid
    label_index = labels[i].item()
    if 0 <= label_index < len(coco_names):
        label = coco_names[label_index]  # Get the name from coco_names
        confidence = scores[i].item()

        # Only draw bounding box if confidence is above a threshold
        if confidence > 0.5:
            print(f"Label: {label}, Confidence: {confidence:.2f}")

            # Draw the bounding box
            cv2.rectangle(image_cv, (x1, y1), (x2, y2), (255, 0, 0), 2)
            # Put the label text
            cv2.putText(image_cv, f"{label}: {confidence:.2f}",
                        (x1, y1 - 10), font, 0.5, (255, 0, 0), 1)

# Display the image with bounding boxes
cv2_imshow(image_cv)  # This is used for displaying images in Google Colab

import cv2
import numpy as np
import torch
import torchvision
from PIL import Image
from google.colab.patches import cv2_imshow
import os

# Load the pretrained Faster R-CNN model (with COCO weights)
model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)
model.eval()  # Set the model to evaluation mode

# COCO category names
coco_names = [
    "__background__", "person", "bicycle", "car", "motorcycle", "airplane", "bus", "train",
    "truck", "boat", "traffic light", "fire hydrant", "N/A", "stop sign", "parking meter",
    "bench", "bird", "cat", "dog", "horse", "sheep", "cow", "elephant", "bear", "zebra",
    "giraffe", "N/A", "backpack", "umbrella", "N/A", "N/A", "handbag", "tie", "suitcase",
    "frisbee", "skis", "snowboard", "sports ball", "kite", "baseball bat", "baseball glove",
    "skateboard", "surfboard", "tennis racket", "bottle", "N/A", "wine glass", "cup", "fork",
    "knife", "spoon", "bowl", "banana", "apple", "sandwich", "orange", "broccoli", "carrot",
    "hot dog", "pizza", "donut", "cake", "chair", "couch", "potted plant", "bed", "N/A",
    "dining table", "N/A", "N/A", "toilet", "N/A", "tv", "laptop", "mouse", "remote", "keyboard",
    "cell phone", "microwave", "oven", "toaster", "sink", "refrigerator", "N/A", "book", "clock",
    "vase", "scissors", "teddy bear", "hair drier", "toothbrush"
]

# Define the directory where your images are stored
image_directory = '/content/drive/MyDrive/Colab Notebooks/Object Recognition Dataset/val2017'  # Replace with your directory path

# Get all image file paths from the directory
image_files = [f for f in os.listdir(image_directory) if f.endswith(('.jpg', '.jpeg', '.png'))]

# Loop through each image in the directory
for image_file in image_files:
    image_path = os.path.join(image_directory, image_file)

    # Load the image
    image = Image.open(image_path)

    # Transformation pipeline to prepare the image for the model
    transform = torchvision.transforms.Compose([
        torchvision.transforms.ToTensor()  # Convert image to tensor
    ])

    img = transform(image).unsqueeze(0)  # Add batch dimension

    # Perform inference
    with torch.no_grad():
        predictions = model(img)

    # Extract bounding boxes, labels, and scores from the predictions
    bboxes = predictions[0]['boxes']
    labels = predictions[0]['labels']
    scores = predictions[0]['scores']

    # Convert the image to a format that OpenCV can display
    image_cv = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)

    # Define font for displaying labels
    font = cv2.FONT_HERSHEY_SIMPLEX

    # Draw bounding boxes on the image
    for i in range(len(bboxes)):
        # Get the bounding box coordinates and convert them to integers
        x1, y1, x2, y2 = bboxes[i].numpy().astype("int")

        # Get the label index and check if it's valid
        label_index = labels[i].item()
        if 0 <= label_index < len(coco_names):
            label = coco_names[label_index]  # Get the name from coco_names
            confidence = scores[i].item()

            # Only draw bounding box if confidence is above a threshold
            if confidence > 0.5:
                print(f"Label: {label}, Confidence: {confidence:.2f} for image {image_file}")

                # Draw the bounding box with a different color for better visibility
                color = (0, 255, 0)  # Green box for detection
                thickness = 2  # Adjust thickness for visibility
                cv2.rectangle(image_cv, (x1, y1), (x2, y2), color, thickness)

                # Put the label text with a background color for clarity
                text = f"{label}: {confidence:.2f}"
                text_color = (0, 0, 255)  # Red text
                text_bg_color = (255, 255, 255)  # White background for text
                padding = 5

                # Get the text size
                text_size = cv2.getTextSize(text, font, 0.5, 1)[0]
                text_width, text_height = text_size

                # Draw the rectangle around the text
                cv2.rectangle(image_cv, (x1, y1 - text_height - padding),
                              (x1 + text_width, y1), text_bg_color, -1)

                # Put the text on the image
                cv2.putText(image_cv, text, (x1, y1 - padding), font, 0.5, text_color, 1)

    # Display the image with bounding boxes
    cv2_imshow(image_cv)  # This is used for displaying images in Google Colab

import os
import cv2
import torch
from torchvision.transforms import functional as F
from torchvision.models.detection import maskrcnn_resnet50_fpn
import matplotlib.pyplot as plt

# COCO class ID to label mapping
COCO_CLASSES = {
    1: "person", 2: "bicycle", 3: "car", 4: "motorcycle", 5: "airplane", 6: "bus",
    7: "train", 8: "truck", 9: "boat", 10: "traffic light", 11: "fire hydrant",
    13: "stop sign", 14: "parking meter", 15: "bench", 16: "bird", 17: "cat",
    18: "dog", 19: "horse", 20: "sheep", 21: "cow", 22: "elephant", 23: "bear",
    24: "zebra", 25: "giraffe", 27: "backpack", 28: "umbrella", 31: "handbag",
    32: "tie", 33: "suitcase", 34: "frisbee", 35: "skis", 36: "snowboard",
    37: "sports ball", 38: "kite", 39: "baseball bat", 40: "baseball glove",
    41: "skateboard", 42: "surfboard", 43: "tennis racket", 44: "bottle",
    46: "wine glass", 47: "cup", 48: "fork", 49: "knife", 50: "spoon", 51: "bowl",
    52: "banana", 53: "apple", 54: "sandwich", 55: "orange", 56: "broccoli",
    57: "carrot", 58: "hot dog", 59: "pizza", 60: "donut", 61: "cake", 62: "chair",
    63: "couch", 64: "potted plant", 65: "bed", 67: "dining table", 70: "toilet",
    72: "TV", 73: "laptop", 74: "mouse", 75: "remote", 76: "keyboard",
    77: "cell phone", 78: "microwave", 79: "oven", 80: "toaster", 81: "sink",
    82: "refrigerator", 84: "book", 85: "clock", 86: "vase", 87: "scissors",
    88: "teddy bear", 89: "hair drier", 90: "toothbrush"
}

# Function to load images from a directory
def load_images_from_directory(directory):
    image_paths = [os.path.join(directory, file) for file in os.listdir(directory) if file.endswith(('jpg', 'jpeg', 'png'))]
    return image_paths

# Function to make predictions on an image
def predict_image(image_path, model, device, threshold=0.5):
    image = cv2.imread(image_path)
    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB for correct prediction
    image_tensor = F.to_tensor(image).unsqueeze(0).to(device)

    model.eval()
    with torch.no_grad():
        predictions = model(image_tensor)

    # Extract predictions above the threshold
    boxes = predictions[0]['boxes'].cpu().numpy()
    labels = predictions[0]['labels'].cpu().numpy()
    scores = predictions[0]['scores'].cpu().numpy()

    # Filter predictions based on the threshold
    boxes = boxes[scores >= threshold]
    labels = labels[scores >= threshold]
    scores = scores[scores >= threshold]

    return image, boxes, labels, scores

# Function to visualize the predictions
def visualize_predictions(image, boxes, labels, scores, category_mapping):
    plt.figure(figsize=(10, 10))
    plt.imshow(image)
    for box, label, score in zip(boxes, labels, scores):
        x1, y1, x2, y2 = box
        plt.gca().add_patch(plt.Rectangle((x1, y1), x2 - x1, y2 - y1, edgecolor='red', fill=False, linewidth=2))
        label_name = category_mapping.get(label, f"Class {label}")
        plt.text(x1, y1 - 10, f"{label_name} ({score:.2f})", color='red', fontsize=12, backgroundcolor='white')
    plt.axis('off')
    plt.show()

# Main Code
if __name__ == "__main__":
    # Directory containing images
    img_dir =  '/content/drive/MyDrive/Colab Notebooks/Object Recognition Dataset/val2017'

    # Pretrained Mask R-CNN model
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = maskrcnn_resnet50_fpn(pretrained=True)
    model.to(device)

    # Load images from directory
    image_paths = load_images_from_directory(img_dir)

    # Loop through images and make predictions
    for image_path in image_paths:
        image, boxes, labels, scores = predict_image(image_path, model, device, threshold=0.5)
        visualize_predictions(image, boxes, labels, scores, COCO_CLASSES)